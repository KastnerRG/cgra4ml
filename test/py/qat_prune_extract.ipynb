{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87f578d",
   "metadata": {},
   "source": [
    "# QAT, Prune, Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02b249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import os\n",
    "\n",
    "import training\n",
    "from quant_layer import QuantConv2d, act_quantization\n",
    "\n",
    "from resnet_quant import ResNet_Cifar\n",
    "from vgg_quant import VGG_quant\n",
    "\n",
    "'''\n",
    "Parameters\n",
    "'''\n",
    "input_size = 32\n",
    "mean = [0.491, 0.482, 0.447]\n",
    "std = [0.247, 0.243, 0.262]\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10\n",
    "batch_size = 128\n",
    "n_epochs_train = 96\n",
    "n_epochs_retrain = 2\n",
    "lr = 0.01\n",
    "adjust_list = [20, 30]\n",
    "optim = torch.optim.SGD\n",
    "device = torch.device(\"cuda\") \n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "prune_ratio = 0.8\n",
    "w_bit = 4\n",
    "x_bit = 4\n",
    "x_alpha = 8.0\n",
    "w_alpha_init = 3.0\n",
    "\n",
    "print_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040e8d6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a607d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_Cifar(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantConv2d(\n",
       "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (weight_quant): weight_quantize_fn()\n",
       "        )\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantConv2d(\n",
       "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (weight_quant): weight_quantize_fn()\n",
       "        )\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (conv2): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (weight_quant): weight_quantize_fn()\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet_Cifar('resnet20_quant', x_bit, w_bit, x_alpha, w_alpha_init)\n",
    "# model = VGG_quant('vgg16_quant', x_bit, w_bit, x_alpha, w_alpha_init)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d05c6",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ed27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "train_dataset = dataset(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(input_size, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = dataset(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ef063",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b442532",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'checkpoints/{model.name}'\n",
    "path = f\"{save_dir}/model_best.pth.tar\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea866ae1",
   "metadata": {},
   "source": [
    "## Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f214bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95][0/391]\tLoss 0.9652 (0.9652)\tPrec 72.656% (72.656%)\n",
      "Epoch: [95][100/391]\tLoss 0.7631 (0.8891)\tPrec 70.312% (68.595%)\n",
      "Epoch: [95][200/391]\tLoss 0.8231 (0.8848)\tPrec 66.406% (68.711%)\n",
      "Epoch: [95][300/391]\tLoss 1.0208 (0.8849)\tPrec 57.812% (68.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tLoss 0.8056 (0.8056)\tPrec 67.969% (67.969%)\n",
      " * Prec 68.250% \n",
      "best acc: 68.250000\n"
     ]
    }
   ],
   "source": [
    "training.train_val_save(\n",
    "    model, \n",
    "    trainloader, \n",
    "    testloader, \n",
    "    optim,\n",
    "    lr,\n",
    "    criterion,\n",
    "    start_epoch,\n",
    "    n_epochs_train, \n",
    "    adjust_list, \n",
    "    print_freq, \n",
    "    save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237827ed",
   "metadata": {},
   "source": [
    "## Pruning & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74272ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.557315\n",
      "Epoch: 2 \tTraining Loss: 1.317897\n",
      "\n",
      "Test set: Accuracy: 5584/10000 (56%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in model.modules():\n",
    "     if isinstance(layer, QuantConv2d):\n",
    "        prune.l1_unstructured(layer, name='weight', amount=prune_ratio)\n",
    "\n",
    "optimizer = optim(model.parameters(), lr=lr)\n",
    "training.simple_train_test(\n",
    "    model, \n",
    "    trainloader, \n",
    "    testloader, \n",
    "    optim,\n",
    "    lr,\n",
    "    criterion, \n",
    "    n_epochs_retrain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052869e8",
   "metadata": {},
   "source": [
    "## Populate x with prehooks into layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95fa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "    def __call__(self, module, module_in):\n",
    "        self.layer.prehooked = module_in[0]\n",
    "        \n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        layer.register_forward_pre_hook(SaveOutput(layer))             \n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f4753",
   "metadata": {},
   "source": [
    "## Calculate integer versions of w, x & y, store them as channel last numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8ad8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        \n",
    "        # Weights\n",
    "        weight_q = layer.weight_q\n",
    "        w_alpha = layer.weight_quant.w_alpha\n",
    "        layer.w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "        layer.w_int = weight_q / layer.w_delta\n",
    "        \n",
    "        # Input\n",
    "        act_quant_fn = act_quantization(x_bit)\n",
    "        x_alpha = layer.x_alpha.item()\n",
    "        layer.x_delta = x_alpha/(2**x_bit-1)\n",
    "        x_q = act_quant_fn(layer.prehooked, x_alpha)\n",
    "        layer.x_int   = x_q/layer.x_delta\n",
    "        \n",
    "        # Output\n",
    "        co, ci, kh, kw = weight_q.shape\n",
    "        layer.y_int = F.conv2d(\n",
    "            layer.x_int, \n",
    "            torch.nn.parameter.Parameter(layer.w_int), \n",
    "            layer.bias, \n",
    "            layer.stride, layer.padding, layer.dilation, layer.groups)\n",
    "        layer.y_f = layer.y_int * layer.w_delta * layer.x_delta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb687f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "conv_i = 0\n",
    "d = {}\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        layer.w_npcl = layer.w_int.cpu().detach().numpy().round().astype(np.int).transpose(2,3,1,0)\n",
    "        layer.x_npcl = layer.x_int.cpu().detach().numpy().round().astype(np.int).transpose(0,2,3,1)\n",
    "        layer.y_npcl = layer.y_int.cpu().detach().numpy().round().astype(np.int).transpose(0,2,3,1)\n",
    "        \n",
    "        conv_i += 1\n",
    "        channel_out = layer.weight.shape[0]\n",
    "        d[f'conv_{conv_i}'] = {\n",
    "            'w': layer.w_npcl, \n",
    "            'x': layer.x_npcl, \n",
    "            'b': np.zeros((channel_out), dtype=np.int8),\n",
    "            'y': layer.y_npcl\n",
    "        }\n",
    "    \n",
    "with open (f'np/np_dict_{model.name}.pickle', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f0a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1 (128, 32, 32, 16)\n",
      "conv_2 (128, 32, 32, 16)\n",
      "conv_3 (128, 32, 32, 16)\n",
      "conv_4 (128, 32, 32, 16)\n",
      "conv_5 (128, 32, 32, 16)\n",
      "conv_6 (128, 32, 32, 16)\n",
      "conv_7 (128, 32, 32, 16)\n",
      "conv_8 (128, 16, 16, 32)\n",
      "conv_9 (128, 32, 32, 16)\n",
      "conv_10 (128, 16, 16, 32)\n",
      "conv_11 (128, 16, 16, 32)\n",
      "conv_12 (128, 16, 16, 32)\n",
      "conv_13 (128, 16, 16, 32)\n",
      "conv_14 (128, 16, 16, 32)\n",
      "conv_15 (128, 8, 8, 64)\n",
      "conv_16 (128, 16, 16, 32)\n",
      "conv_17 (128, 8, 8, 64)\n",
      "conv_18 (128, 8, 8, 64)\n",
      "conv_19 (128, 8, 8, 64)\n",
      "conv_20 (128, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "for k, l in d.items():\n",
    "    print(k,l['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a8e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 16, 16)\n",
      "(128, 32, 32, 16)\n",
      "(128, 32, 32, 16)\n"
     ]
    }
   ],
   "source": [
    "print(d['conv_5']['w'].shape)\n",
    "print(d['conv_5']['x'].shape)\n",
    "print(d['conv_5']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0ee1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = list(model.modules())[2]\n",
    "\n",
    "# conv_ref = torch.nn.Conv2d(in_channels=layer.in_channels, \n",
    "#                             out_channels=layer.out_channels, \n",
    "#                             kernel_size=layer.kernel_size, \n",
    "#                             stride=layer.stride, \n",
    "#                             padding=layer.padding, \n",
    "#                             dilation=layer.dilation, \n",
    "#                             groups=layer.groups, \n",
    "#                             bias=layer.bias)\n",
    "\n",
    "# weight = layer.weight\n",
    "# # mean = 0\n",
    "# # std = 1\n",
    "# mean = weight.data.mean()\n",
    "# std = weight.data.std()\n",
    "# conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "# output_ref = conv_ref(layer.prehooked)\n",
    "\n",
    "# difference = abs(output_ref - layer.y_f )\n",
    "# print(difference.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
