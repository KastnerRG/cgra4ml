{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ABy3xAE8uW__"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n","c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["from qkeras import *\n","from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Softmax, Add, ZeroPadding2D, MaxPooling2D\n","import numpy as np\n","from collections import namedtuple\n","import pickle\n","import math\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.utils import to_categorical\n","from qkeras.utils import model_save_quantized_weights"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vG3iXUBnuXAB"},"outputs":[],"source":["def load_data(num_classes=10, subtract_pixel_mean=True):\n","    \"\"\"\n","    Load CIFAR10 data and normalize\n","    \"\"\"\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    # input image dimensions.\n","    input_shape = x_train.shape[1:]\n","\n","    # normalize data.\n","    x_train = x_train.astype('float32') / 128.0 - 1.0\n","    x_test = x_test.astype('float32') / 128.0 - 1.0\n","\n","    # if subtract pixel mean is enabled\n","    if subtract_pixel_mean:\n","        x_train_mean = np.mean(x_train, axis=0)\n","        x_train -= x_train_mean\n","        x_test -= x_train_mean\n","\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","    print('y_train shape:', y_train.shape)\n","\n","    # convert class vectors to binary class matrices,\n","    # i.e., one hot encodings\n","    y_train = to_categorical(y_train, num_classes)\n","    y_test = to_categorical(y_test, num_classes)\n","\n","    return x_train, y_train, x_test, y_test\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtMyMdKTuXAC","outputId":"24e6c7d8-b3bc-4b07-a12f-3d3955194087"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n","y_train shape: (50000, 1)\n"]}],"source":["x_train, y_train, x_test, y_test = load_data(10, False)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FtdLlJbzuXAC"},"outputs":[],"source":["input_shape = x_train.shape[1:-1] + (3,)\n","np.random.seed(1)\n","\n","a_0 = 'quantized_relu(8,0,negative_slope=0.125)'\n","a_1 = 'quantized_relu(8,1,negative_slope=0.125)'\n","a_2 = 'quantized_relu(8,2,negative_slope=0.125)'\n","a_3 = 'quantized_relu(8,3,negative_slope=0.125)'\n","\n","q_0 = 'quantized_bits(8,0,False,True,1)'\n","q_1 = 'quantized_bits(8,1,False,True,1)'\n","q_2 = 'quantized_bits(8,2,False,True,1)'\n","q_3 = 'quantized_bits(8,3,False,True,1)'\n","\n","q_t = 'quantized_bits(8,0,False,True,1)'\n","\n","np.random.seed(42)\n","#preamble = './drive/MyDrive/resnet/'\n","preamble = ''\n","USE_BIAS = True"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qgoMo_ima_OB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input (InputLayer)             [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," q_activation (QActivation)     (None, 32, 32, 3)    0           ['input[0][0]']                  \n","                                                                                                  \n"," bundle (Bundle)                (None, 16, 16, 64)   9729        ['q_activation[0][0]']           \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_1 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm (QConv2DBat  multiple          9729        []                               |\n","| chnorm)                                                                                        |\n","|                                                                                                |\n","| q_activation_2 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| max_pooling2d (MaxPooling2D)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_1 (Bundle)              (None, 16, 16, 64)   37185       ['bundle[0][0]']                 \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_3 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_1 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_2 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_1[0][0]',               \n","                                                                  'bundle[0][0]']                 \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_4 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_2 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_5 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_3 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_2[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_6 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_3 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_4 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_3[0][0]',               \n","                                                                  'bundle_2[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_7 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_4 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_8 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_5 (Bundle)              (None, 8, 8, 128)    74369       ['bundle_4[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_9 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_5 (QConv2DB  multiple          74369       []                               |\n","| atchnorm)                                                                                      |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_6 (Bundle)              (None, 8, 8, 128)    148097      ['bundle_5[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_10 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_6 (QConv2DB  multiple          148097      []                               |\n","| atchnorm)                                                                                      |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_7 (Bundle)              (None, 8, 8, 128)    8833        ['bundle_4[0][0]',               \n","                                                                  'bundle_6[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_11 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_7 (QConv2DB  multiple          8833        []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_12 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_8 (Bundle)              (None, 8, 8, 128)    148097      ['bundle_7[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_13 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_8 (QConv2DB  multiple          148097      []                               |\n","| atchnorm)                                                                                      |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_9 (Bundle)              (None, 8, 8, 128)    17025       ['bundle_8[0][0]',               \n","                                                                  'bundle_7[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_14 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_9 (QConv2DB  multiple          17025       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_15 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_10 (Bundle)             (None, 4, 4, 256)    296193      ['bundle_9[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_16 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_10 (QConv2D  multiple          296193      []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_11 (Bundle)             (None, 4, 4, 256)    591105      ['bundle_10[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_17 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_11 (QConv2D  multiple          591105      []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_12 (Bundle)             (None, 4, 4, 256)    34049       ['bundle_9[0][0]',               \n","                                                                  'bundle_11[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_18 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_12 (QConv2D  multiple          34049       []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_19 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_13 (Bundle)             (None, 4, 4, 256)    591105      ['bundle_12[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_20 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_13 (QConv2D  multiple          591105      []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_14 (Bundle)             (None, 4, 4, 256)    66817       ['bundle_13[0][0]',              \n","                                                                  'bundle_12[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_21 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_14 (QConv2D  multiple          66817       []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_22 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_15 (Bundle)             (None, 2, 2, 512)    1182209     ['bundle_14[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_23 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_15 (QConv2D  multiple          1182209     []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_16 (Bundle)             (None, 2, 2, 512)    2361857     ['bundle_15[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_24 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_16 (QConv2D  multiple          2361857     []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_17 (Bundle)             (None, 2, 2, 512)    133633      ['bundle_14[0][0]',              \n","                                                                  'bundle_16[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_25 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_17 (QConv2D  multiple          133633      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_26 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_18 (Bundle)             (None, 2, 2, 512)    2361857     ['bundle_17[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_27 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_18 (QConv2D  multiple          2361857     []                               |\n","| Batchnorm)                                                                                     |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_19 (Bundle)             (None, 512)          264705      ['bundle_18[0][0]',              \n","                                                                  'bundle_17[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_28 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_conv2d_batchnorm_19 (QConv2D  multiple          264705      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_29 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_30 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_average_pooling2d (QAverageP  multiple          0           []                               |\n","| ooling2D)                                                                                      |\n","|                                                                                                |\n","| flatten (Flatten)            multiple             0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_20 (Bundle)             (None, 10)           5130        ['bundle_19[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_activation_31 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_dense (QDense)             multiple             5130        []                               |\n","|                                                                                                |\n","| activation (Activation)      multiple             0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n","==================================================================================================\n","Total params: 8,443,550\n","Trainable params: 8,433,930\n","Non-trainable params: 9,620\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["class Bundle(tf.keras.Model):\n","    def __init__(self, \n","                 core,             # dict, Mandaroty: parameters for conv/dense layer, act can be quantization or relu\n","                 add=None,         # dict, Mandatory if x1 is not None in call(), else ignored\n","                 pool=None,        # dict, Optional: can only be max or avg\n","                 flatten=False,    # Optional: set to True to flatten the outputs\n","                 softmax=False,    # Optional: set to Ture to include floating point softmax layer\n","                 **kwargs):\n","\n","        super(Bundle, self).__init__()\n","        \n","        self.core = core\n","        self.add = add\n","        self.pool = pool\n","        self.flatten = flatten\n","        self.softmax = softmax\n","        self.inp = {'tensor':None, 'int': None, 'bits':None, 'frac': None}\n","        self.out = {'tensor':None, 'int': None, 'bits':None, 'frac': None}\n","        self.proc = {'tensor':None, 'int': None, 'bits':None, 'frac': None}\n","        self.w = {'tensor':None, 'int': None, 'bits':None, 'frac': None}\n","        self.b = {'tensor':None, 'int': None, 'bits':None, 'frac': None}\n","\n","        # Store reference to bundle object here, not just a idx number\n","        self.prev_bundle = None\n","        self.add_bundle = None\n","\n","        def extract_act(signature):\n","            ilayer = QActivation(signature)\n","            d = ilayer.quantizer.get_config()\n","            sign_bit = d['keep_negative'] if 'keep_negative' in d else (d['negative_slope'] !=0 if 'negative_slope' in d else (0))\n","            int_bit = d['integer'] if 'integer' in d else 0\n","            frac = d['bits']-int_bit-sign_bit\n","\n","            if isinstance(ilayer.quantizer, quantized_bits):\n","                return { 'layer':ilayer, 'type':'quant', 'bits':d['bits'], 'frac':frac}\n","            elif 'relu' in str(ilayer.quantizer.__class__) and ilayer.quantizer.negative_slope != 0:\n","                return { 'layer':ilayer, 'type':'relu', 'slope':ilayer.quantizer.negative_slope, 'bits':d['bits'], 'frac':frac}\n","            else:\n","                raise Exception(\"Only leaky_relu (relu with negative_slope > 0) is suppported!\")\n","\n","        '''\n","        CORE LAYER\n","        '''\n","        if core['type'] == 'conv':\n","            for i in ['filters', 'kernel_size', 'strides', 'padding', 'kernel_quantizer', 'bias_quantizer', 'use_bias', 'act_str']:\n","                assert i in core, f\"'{i}' must be provided for conv\"\n","\n","            self.core['layer'] = QConv2DBatchnorm(\n","                filters=self.core['filters'], kernel_size=self.core['kernel_size'], strides=self.core['strides'],\n","                padding=self.core['padding'], kernel_quantizer=self.core['kernel_quantizer'], \n","                bias_quantizer=self.core['bias_quantizer'], use_bias=self.core['use_bias'])\n","        \n","        else:\n","            for i in ['units', 'kernel_quantizer', 'bias_quantizer', 'use_bias', 'act_str']:\n","                assert i in self.core, f\"'{i}' must be provided for dense\"\n","            \n","            self.core['layer'] = QDense(\n","                units=self.core['units'], kernel_quantizer=self.core['kernel_quantizer'],\n","                bias_quantizer=self.core['bias_quantizer'], use_bias=self.core['use_bias'])\n","\n","        '''\n","        CORE ACT LAYER\n","        '''\n","        self.core['act'] = extract_act(core['act_str'])\n","        self.out['frac'], self.out['bits'] = self.core['act']['frac'], self.core['act']['bits']\n","\n","        '''\n","        ACT ADD LAYER\n","        '''\n","        if self.add is not None:\n","            self.add['act'] = extract_act(add['act_str'])\n","            self.out['frac'], self.out['bits'] = self.add['act']['frac'], self.add['act']['bits']\n","\n","        '''\n","        POOL LAYER\n","        '''\n","        if pool:\n","            for i in ['type', 'size', 'strides', 'padding']:\n","                assert i in pool, f\"'{i}' must be provided for pool\"\n","\n","            if pool['type'] == 'max':\n","                self.pool_layer = MaxPooling2D(self.pool['size'], strides=self.pool['strides'], padding=self.pool['padding'])\n","            elif pool['type'] == 'avg':\n","                self.pool_layer = QAveragePooling2D(self.pool['size'], strides=self.pool['strides'], padding=self.pool['padding'])\n","            else:\n","                raise Exception(self.pool['type'], \"only avg or max pool is supported for now\")\n","            \n","            self.pool['act'] = extract_act(self.pool['act_str'])\n","            self.out['frac'], self.out['bits'] = self.pool['act']['frac'], self.pool['act']['bits']\n","        else:\n","            self.pool_layer = None\n","\n","        '''\n","        FLATTEN & SOFTMAX LAYERS\n","        '''\n","        self.flatten_layer = Flatten() if self.flatten else None\n","\n","        self.softmax = softmax\n","        self.softmax_layer = Activation(\"softmax\") if self.softmax else None\n","        if softmax:\n","            self.out['frac'], self.out['bits'] = 0, 1\n","\n","\n","    # functions for training\n","    def call(self, x, x_1=None):\n","        if hasattr(x, \"bundle\"):\n","            self.prev_bundle = x.bundle\n","            self.idx = self.prev_bundle.idx + 1\n","        else:\n","            self.prev_bundle = None\n","            self.idx = 0\n","\n","        self.inp['tensor'] = x\n","\n","        x = self.core['layer'](x)\n","        x = self.core['act']['layer'](x)\n","        self.core['tensor'] = x\n","\n","        if x_1 is not None:\n","            if hasattr(x_1, \"bundle\"):\n","                self.add['bundle'] = x_1.bundle\n","            else:\n","                self.add['bundle'] = None\n","            x = Add()([x, x_1])\n","            x = self.add['act']['layer'](x)\n","            self.add['tensor'] = x\n","        if self.pool_layer:\n","            x = self.pool_layer(x)\n","            x = self.pool['act']['layer'](x)\n","            self.pool['tensor'] = x\n","        if self.flatten_layer:\n","            x = self.flatten_layer(x)\n","        if self.softmax_layer:\n","            x = self.softmax_layer(x)\n","\n","        self.out['tensor'] = x\n","        x.bundle = self\n","        return x\n","\n","    # functions to be prepared for exportation\n","    def load_weight_bias(self):\n","        k_tensor = self.core['layer'].get_folded_weights()[0] if isinstance(self.core['layer'], QConv2DBatchnorm) else self.core['layer'].kernel\n","        k = self.core['layer'].kernel_quantizer_internal(k_tensor).numpy()\n","        k_config = self.core['layer'].kernel_quantizer_internal.get_config()\n","\n","        k_frac = k_config['bits']-k_config['integer']-k_config['keep_negative']\n","        k_int = k * 2**k_frac\n","        assert (k_int == k_int.astype(int)).all(), f\"Weights failed integer test for bundle {self.idx}\"\n","        k_int = k_int.astype(int)\n","        self.w = {'tensor':k_tensor, 'int': k_int, 'bits':k_config['bits'], 'frac':k_frac}\n","\n","        if (self.core['type'] == 'conv' and self.core['use_bias']) or (self.core['type'] == 'dense' and self.core['use_bias']):\n","            b_tensor = self.core['layer'].get_folded_weights()[1] if isinstance(self.core['layer'], QConv2DBatchnorm) else self.core['layer'].bias\n","            b = self.core['layer'].bias_quantizer_internal(b_tensor).numpy()\n","            b_config = self.core['layer'].bias_quantizer_internal.get_config()\n","            b_frac = b_config['bits']-b_config['integer']-b_config['keep_negative']\n","            b_int = b * 2**b_frac\n","            assert (b_int == b_int.astype(int)).all(), f\"Bias failed integer test for bundle {self.idx}\"\n","            b_int = b_int.astype(int)\n","            self.b = {'tensor':b_tensor, 'int':b_int, 'bits':b_config['bits'], 'frac':b_frac}\n","\n","\n","    def process(self, inp = None):\n","        \n","        ''' Integer test for output '''\n","        self.out['int'] = self.out['tensor'].numpy() * 2**self.out['frac']\n","        if self.softmax is None:\n","            assert (self.out['int'] == self.out['int'].astype(int)).all(), f\"Output tensor of bundle {self.idx} is not a fixed point\"\n","            self.out['int'] = self.out['int'].astype(int)\n","\n","        if inp is not None: # independant mode\n","            self.inp = inp\n","        else: # chained mode\n","            # ToDo: do not rely on external(global) variables!\n","            self.inp = self.prev_bundle.out\n","            assert self.idx > 0, \"input must be provided manually for the first bundle\"\n","\n","        self.load_weight_bias()\n","        x = self.inp['int'].astype(np.int32)\n","        w = self.w['int'].astype(np.int32)\n","\n","        if self.core['type'] == 'conv':\n","            self.proc['int'] = tf.keras.backend.conv2d(x, w, padding='same').numpy()\n","        else:\n","            self.proc['int'] = x @ w\n","\n","        self.post_process()\n","\n","\n","    def post_process(self):\n","        \n","        clog2_add = int(np.ceil(np.log2(np.prod(self.w['int'].shape[:-1]))))\n","        self.proc['bits'] = self.inp['bits'] + self.w['bits'] + clog2_add\n","        self.proc['frac'] = self.inp['frac'] + self.w['frac']\n","\n","        if self.b is not None:\n","            self.proc['int'] += self.b['int'] * 2** (self.proc['frac'] - self.b['frac'])\n","\n","\n","        if 'strides' in self.core and self.core['strides'] != (1,1):\n","            SH, SW = self.core['strides']\n","            N, XH, XW, C = self.proc['int'].shape\n","            YH, YW = XH//SH, XW//SW\n","            self.proc['int'] = self.proc['int'].reshape(N, YH, SH, YW, SW, C)\n","            ind = -1 if self.w['int'].shape[0] > 1 else 0\n","            self.proc['int'] = self.proc['int'][:,:,ind,:,ind,:]\n","\n","        def apply_act(act_dict):\n","            x = self.proc['int'].astype(np.float32)\n","            frac, bits = act_dict['frac'], act_dict['bits']\n","\n","            if act_dict['type'] == 'quant':\n","                x *= 2**(frac-self.proc['frac'])\n","                x = np.around(x)\n","                x = np.clip(x, -2**(bits-1), 2**(bits-1)-1).astype(int)\n","\n","            elif act_dict['type'] == 'relu':\n","                x *= 2**(frac-self.proc['frac'])\n","                x = np.clip(x, -2**(bits-1), 2**(bits-1)-1)\n","                x = np.maximum(x * act_dict['slope'], x)\n","                x = np.around(x)\n","                x = np.clip(x,-2**(bits-1), 2**(bits-1)-1).astype(int)\n","            else:\n","                raise Exception('Only relu is supported yet')\n","\n","            self.proc['int'], self.proc['bits'], self.proc['frac'] = x, bits, frac\n","\n","        apply_act(self.core['act'])\n","        assert np.all(self.proc['int'] == self.core['tensor'].numpy() * 2**self.proc['frac']), f\"Core + act output of bundle {self.idx} is not fixed point\"\n","\n","\n","        if self.add is not None:\n","            a = self.add['bundle']\n","            out_frac_add, out_bits_add = max(self.proc['frac'], a.out['frac']), max(self.proc['bits'], a.out['bits'])\n","\n","            a_arr_cast = a.out['int'] * 2** (out_frac_add - a.out['frac'])\n","            out_arr_cast = self.proc['int'] * 2 **(out_frac_add - self.proc['frac'])\n","\n","            self.proc['int'] = out_arr_cast.astype(np.int64) + a_arr_cast.astype(np.int64)\n","            self.proc['bits'], self.proc['frac'] = out_bits_add, out_frac_add\n","            apply_act(self.add['act'])\n","\n","            assert np.all(self.proc['int'] == self.add['tensor'].numpy() * 2**self.proc['frac']), f\"Add + act output of bundle {self.idx} is not a fixed point\"\n","\n","        if self.pool_layer:\n","            if self.pool['type'] == 'max':\n","                pStride = self.pool['strides']\n","                pSize = self.pool['size']\n","\n","                def findMax(InArray, p, q):\n","                    results = np.zeros((InArray.shape[0], InArray.shape[3]))\n","                    results -= math.inf\n","                    for i in range(p, p+pSize[0]):\n","                        for j in range(q, q+pSize[1]):\n","                            if i >=0 and j>=0 and i < InArray.shape[1] and j < InArray.shape[2]:\n","                                cand = InArray[:,i,j,:]\n","                                results = np.maximum(results, cand)\n","                    return results\n","\n","                def HotFixMaxPool2D(InArray):\n","                    if pStride[0]!=pStride[1] or pSize[0]!=pSize[1]:\n","                        raise Exception('Only square stride and size is supported')\n","                    if pSize[0]/2 == 0:\n","                        raise Exception('Maxpool size should be odd')\n","\n","                    pad = (pSize[0]-1)//2\n","\n","                    inShape = InArray.shape\n","                    assert len(inShape) == 4\n","                    OutArray = np.zeros((inShape[0], inShape[1]//pStride[0], inShape[2]//pStride[1], inShape[3]))\n","                    # Start point, should include pad\n","                    st_p, st_q = -pad, -pad\n","\n","                    for i in range(OutArray.shape[1]):\n","                        for j in range(OutArray.shape[2]):\n","                            p, q = st_p + i*pStride[0] + pStride[0]-1, st_q + j*pStride[1] + pStride[1]-1\n","                            OutArray[:,i,j,:] = findMax(InArray, p, q)\n","\n","                    return OutArray\n","\n","                self.proc['int'] = HotFixMaxPool2D(self.proc['int']).astype(int)\n","\n","            elif self.pool['type'] == 'avg':\n","                assert self.pool['size'] == self.pool['strides']\n","                KH, KW = self.pool['size']\n","                N, H, W, C = self.proc['int'].shape\n","                self.proc['int'] = self.proc['int'].reshape(N, H//KH, KH, W//KW, KW, C).mean(axis=(2,4))\n","                # NO need for clipping, as act_pool in place!\n","                apply_act(self.pool['act'])\n","            assert np.all(self.proc['int'] == self.pool['tensor'].numpy() * 2**self.proc['frac']), f\"Pool + act output of bundle {self.idx} is not a fixed point\"\n","\n","        if self.flatten:\n","            self.proc['int'] = self.proc['int'].reshape(self.proc['int'].shape[0],-1)\n","\n","\n","        if self.softmax:\n","            self.proc['int'] = self.proc['int'] / 2**self.proc['frac']\n","            exp = np.exp(self.proc['int'] - self.proc['int'].max())\n","            self.proc['int'] = exp/np.sum(exp, axis=1)[0]\n","            assert np.all(np.argmax(self.out['int'], axis=-1) == np.argmax(self.proc['int'], axis=-1))\n","        else:\n","            assert np.all(self.proc['int'] == self.out['int']), f\"Overall output of bundle {self.idx} is not a fixed point\"\n","\n","    @staticmethod\n","    def get_compile_params(bundles, ROWS, COLS):\n","\n","        def clog2(x):\n","            return int(np.ceil(np.log2(x)))\n","        \n","        IN_BITS               = 64\n","        CONFIG_BEATS          = 1\n","        X_BITS = K_BITS       = max([b.x[1] for b in bundles])\n","        KW_MAX                = max([b.KW   for b in bundles])\n","        KH_MAX                = max([b.KH   for b in bundles])\n","        SW_MAX                = max([b.SW   for b in bundles])\n","        SH_MAX                = max([b.SH   for b in bundles])\n","        CI_MAX                = max([b.CI   for b in bundles])\n","        XW_MAX                = max([b.XW   for b in bundles])\n","        XH_MAX                = max([b.XH   for b in bundles])\n","        XN_MAX                = max([b.XN   for b in bundles])\n","        BRAM_WEIGHTS_DEPTH    = max([b.RAM_WEIGHTS + CONFIG_BEATS for b in bundles])\n","        RAM_EDGES_DEPTH       = max([b.RAM_EDGES                  for b in bundles])\n","        \n","        L_MAX                 = clog2(XH_MAX//ROWS)\n","        X_PAD                 = clog2(KH_MAX//2)\n","        BITS_KW2              = clog2((KW_MAX+1)/2)\n","        BITS_KH2              = clog2((KH_MAX+1)/2)\n","        BITS_SW               = clog2(SW_MAX)\n","        BITS_SH               = clog2(SH_MAX)\n","        BITS_CIN_MAX          = clog2(CI_MAX)\n","        BITS_COLS_MAX         = clog2(XW_MAX)\n","        BITS_BLOCKS_MAX       = clog2( L_MAX)\n","        BITS_XN_MAX           = clog2(XN_MAX)\n","        BITS_BRAM_WEIGHTS_ADDR= clog2(BRAM_WEIGHTS_DEPTH)\n","\n","        params = locals()\n","        params = {k:params[k] for k in params if not ('__' in k or k in ['bundles', 'params', 'clog2'])}\n","        c = namedtuple('Compile', params)(**params)\n","        return c\n","\n","    def export (self, c):\n","\n","        if self.core['type'] != 'conv':\n","            print('Conv -> Dense Reshape')\n","            CI, CO = self.w['int'].shape\n","            XN, _ = self.inp['int'].shape\n","            w_int = self.w['int'].reshape(1,1,CI,CO) # (CI,CO) -> (KH,KW,CI,CO)\n","            x_int = self.inp['int'].reshape(XN,1,1,CI) # (XN,CI) -> (XN, XH, XW, CI)\n","            y_int = self.out['int'].reshape(XN,1,1,CO) # (XN,CI) -> (XN, XH, XW, CI)\n","        else:\n","            if self.flatten:\n","                XN, CO = self.out['int'].shape\n","                y_int = self.out['int'].reshape(XN,1,1,CO) # (XN,CI) -> (XN, XH, XW, CI)\n","            else:\n","                y_int = self.out['int']\n","            w_int, x_int = self.w['int'], self.inp['int']\n","        \n","        self.r = self.get_runtime_params(c, w_int.shape, x_int.shape, y_int.shape)\n","        self.r = self.create_headers(c, self.r)\n","\n","        print(self.r)\n","        self.check_sparsity(w_int, x_int)\n","\n","        self.we = self.reorder_w_q2e_conv(w_int, c, self.r)\n","        self.ye_exp_shape = (self.r.IT, self.r.XN, self.r.L, self.r.XW*self.r.CO_PRL, c.ROWS)\n","        self.ye_hw = np.zeros(self.ye_exp_shape)\n","        self.num_t = self.we.shape[0] # iterations\n","\n","        self.r = self.r._asdict()\n","        self.c = c._asdict()\n","\n","    @staticmethod\n","    def get_runtime_params(c, w_shape, x_shape, y_shape):\n","\n","        SW = SH = 1 # for bundle\n","        KH, KW, CI, CO = w_shape\n","        print('weights initial (KH, KW, CI, CO) =', w_shape)\n","\n","        CO_PRL         = c.COLS * SW // KW                        # SW cols are processed in parallel\n","        EG             = int(np.floor( c.COLS / (KW + SW - 1)))   # elastic groups\n","        IT             = int(np.ceil( CO / (SW*EG)))              # iterations needed\n","        CO_PAD         = IT * CO_PRL                              # output cols padded\n","\n","        print(f'{KH=}, {KW=}, {CI=}, {CO=}, {CO_PRL=}, {EG=}, {IT=}, {CO_PAD}')\n","\n","        XN, XH, XW, CI = x_shape\n","        print('input initial (XN, XH, XW, CI)=', x_shape)\n","        print('output initial', y_shape)\n","        SH_OUT, SW_OUT = x_shape[1]//y_shape[1], x_shape[2]//y_shape[2]\n","\n","        LH     = c.ROWS*SH              # Block height\n","        L      = int(np.ceil(XH/LH))    # Blocks\n","        XH_PAD = LH*L\n","        BRAM_WEIGHTS_ADDR_MAX  = c.CONFIG_BEATS + SW*KH*CI-1\n","\n","        '''\n","        Pack all local variables into a namedtuple\n","        '''\n","        params = locals()\n","        params = {k:params[k] for k in params if not ('__' in k or k in ['w', 'x', 'y', 'c', 'params'])}\n","        print (params)\n","        r = namedtuple('Runtime', params)(**params)\n","        return r\n","\n","\n","    @staticmethod\n","    def create_headers(c, r):\n","        '''\n","        Create headers\n","        '''\n","        def pack_bits(arr):\n","            sum_width = 0\n","            packed = 0\n","            for val, width in arr:\n","                packed |= val << sum_width\n","                sum_width += width\n","            return packed\n","        \n","        ''' Weights Config'''\n","        w_config = pack_bits([\n","            (r.KW//2, c.BITS_KW2),\n","            (r.CI-1 , c.BITS_CIN_MAX),\n","            (r.XW-1 , c.BITS_COLS_MAX),\n","            (r.L -1 , c.BITS_BLOCKS_MAX),\n","            (r.XN-1 , c.BITS_XN_MAX),\n","            (r.BRAM_WEIGHTS_ADDR_MAX, c.BITS_BRAM_WEIGHTS_ADDR)\n","        ])\n","        w_config = format(w_config, f'#0{c.IN_BITS}b')\n","        w_config_words = [int(w_config[i:i+c.K_BITS], 2) for i in range(0, len(w_config), c.K_BITS)]\n","        w_config_words.reverse()\n","        w_config_words = np.array(w_config_words,dtype=np.int8)\n","        w_config_words = np.repeat(w_config_words[np.newaxis,...],repeats=r.IT,axis=0)\n","\n","        '''Input Config'''\n","        x_config = pack_bits([\n","            (r.KH//2, c.BITS_KH2),\n","            (r.CI-1 , c.BITS_CIN_MAX),\n","            (r.XW-1 , c.BITS_COLS_MAX),\n","            (r.L -1 , c.BITS_BLOCKS_MAX),\n","        ])\n","        assert c.IN_BITS >= c.BITS_KW2 + c.BITS_CIN_MAX + c.BITS_COLS_MAX + c.BITS_BLOCKS_MAX\n","\n","        x_config = format(x_config, f'#0{c.IN_BITS}b')\n","        x_config_words = [int(x_config[i:i+c.X_BITS], 2) for i in range(0, len(x_config), c.X_BITS)]\n","        x_config_words.reverse()\n","\n","        d = {'w_config':w_config, 'w_config_words':w_config_words, 'x_config':x_config, 'x_config_words': x_config_words}\n","        n = namedtuple('Runtime', d)(**d)\n","        r = namedtuple(\"Runtime\", r._fields + n._fields)(*(r + n))\n","        return r\n","\n","\n","    @staticmethod\n","    def check_sparsity(w, x):\n","        w_sparse = (w==0).sum()/w.size\n","        x_sparse = (x==0).sum()/x.size\n","\n","        p_both_zero = x_sparse * w_sparse\n","        p_only_one_zero = (1-x_sparse) * w_sparse  +  (1-w_sparse) * x_sparse\n","        p_neither_zero = (1-x_sparse) * (1-w_sparse)\n","        zero_result = 1-p_neither_zero\n","\n","        print(f'''\n","        w_sparsity   : {w_sparse*100:.2f}%\n","        x_sparsity   : {x_sparse*100:.2f}%\n","\n","        both_zero    : {p_both_zero*100:.2f}%\n","        only_one_zero: {p_only_one_zero*100:.2f}%\n","        neither_zero : {p_neither_zero*100:.2f}%\n","        zero_result  : {zero_result*100:.2f}%\n","        ''')\n","\n","\n","    @staticmethod\n","    def reorder_w_q2e_conv(w, c, r):\n","\n","        w = np.pad(w, ((0,0),(0,0),(0,0),(0,r.CO_PAD-r.CO)))        # (KH, KW, CI, CO_PAD)\n","        print(w.shape, (r.KH, r.KW, r.CI, r.IT, r.CO_PRL))\n","        w = w.reshape(r.KH, r.KW, r.CI, r.IT, r.CO_PRL)             # (KH, KW, CI, IT, CO_PRL)\n","        w = np.flip(w, axis=4)\n","        w = w.transpose(0,2,3,4,1)                                  # (KH, CI, IT, CO_PRL, KW)\n","\n","        w = w.reshape  (r.KH, r.CI, r.IT, r.CO_PRL*r.KW)            # (KH, CI, IT, CO_PRL*KW)\n","        w = np.pad(w, ((0,0),(0,0),(0,0),(0,c.COLS-r.CO_PRL*r.KW))) # (KH, CI, IT, c.COLS)\n","        w = w.transpose(2,1,0,3)                                    # (IT, CI, KH, c.COLS)\n","        w = w.reshape (r.IT, r.CI*r.KH, c.COLS)                       # (IT, CI*KH, c.COLS)\n","        \n","        w = np.pad(w, ((0,0),(c.CONFIG_BEATS,0),(0,0)))             # (IT, c.CONFIG_BEATS+CI*KH, c.COLS)\n","        w = w.reshape (r.IT, (r.CI*r.KH+c.CONFIG_BEATS)*c.COLS)     # (IT, (CI*KH+c.CONFIG_BEATS)*c.COLS)\n","\n","        w = np.concatenate([r.w_config_words, w], axis=1)             # (IT, 8 + CI*KH*c.COLS)\n","        assert w.shape == (r.IT, c.IN_BITS/c.K_BITS + (r.CI*r.KH+c.CONFIG_BEATS)*c.COLS)\n","        return w\n","\n","\n","    @staticmethod\n","    def reorder_x_q2e_conv(x, c, r):\n","        print('input initial (XN, XH, XW, CI)=', x.shape)\n","\n","        x = np.pad(x, ((0,0),(0,r.XH_PAD-r.XH),(0,0),(0,0)))   # (XN, L*HL , XW, CI)\n","        x = x.reshape  (r.XN, r.L, r.LH, r.XW, r.CI)               # (XN, L, HL, XW, CI)\n","\n","        zeros = np.zeros((r.XN,r.L,c.ROWS+c.X_PAD,r.XW,r.CI),x.dtype)  # (XN,L,c.ROWS+X_PAD,XW,CI)\n","        zeros[:,:,:c.ROWS,:,:] = x\n","\n","        ''' Fill bot rows from next '''\n","        for l in range(r.L):\n","            if l == r.L-1:\n","                zeros[:,l, c.ROWS: ,:,:] = np.zeros((r.XN,c.X_PAD,r.XW,r.CI),x.dtype)\n","            else:\n","                zeros[:,l, c.ROWS: ,:,:] = x[:,l+1,:c.X_PAD,:,:]\n","\n","        x = zeros                  # (XN,L,c.ROWS+X_PAD,XW,CI)\n","        x = x.transpose(0,1,3,4,2) # (XN,L,XW,CI,c.ROWS+X_PAD)\n","\n","        x = x.reshape((r.XN*r.L*r.XW*r.CI*(c.ROWS+c.X_PAD)))\n","        x = np.concatenate([np.array(r.x_config_words, dtype=np.uint8), x.flatten()])\n","        assert x.shape == (c.IN_BITS/c.X_BITS + r.XN*r.L*r.XW*r.CI*(c.ROWS+c.X_PAD),)\n","        return x\n","\n","\n","    @staticmethod\n","    def reorder_y_q2e_conv(y, c, r):\n","        YH, YW = r.XH_PAD//r.SH_OUT, r.XW//r.SW_OUT\n","\n","        if r.SH_OUT != 1:\n","            print(\"Striding not yet supported\")\n","            return None\n","\n","        y = np.pad(y, ((0,0),(0,r.LH*r.L-r.XH),(0,0),(0,r.CO_PAD-r.CO)))     # (XN, L*HL , XW, CO_PAD)\n","        y = y.reshape((r.XN, r.L, c.ROWS, r.XW, r.CO_PAD))                   # (XN,L,c.ROWS,XW,CO_PAD)\n","        y = y.reshape((r.XN, r.L, c.ROWS, r.XW, r.IT, r.CO_PRL))             # (XN,L,c.ROWS,XW,IT,CO_PRL)\n","        y = y.transpose(4,0,1,3,5,2)                                         # (IT,XN,L,XW,CO_PRL,c.ROWS)\n","\n","        assert y.shape == (r.IT,r.XN,r.L,r.XW,r.CO_PRL,c.ROWS)\n","\n","        y_w_last = y[:,:,:,-(r.KW//2+1):,:,:]\n","        y_w_last = y_w_last.transpose(0,1,2,4,3,5).reshape(r.IT,r.XN,r.L,(r.KW//2+1)*r.CO_PRL,c.ROWS)\n","\n","        y = y.reshape(r.IT,r.XN,r.L,r.XW*r.CO_PRL,c.ROWS)\n","        y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:] = y_w_last\n","        return y\n","    \n","    @staticmethod\n","    def reorder_y_e2q_conv(y, c, r):\n","        y = y.reshape(r.IT,r.XN,r.L,r.XW*r.CO_PRL,c.ROWS)\n","\n","        y_w_last = y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:]\n","        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,r.CO_PRL,(r.KW//2+1),c.ROWS)\n","        y_w_last = y_w_last.transpose(0,1,2,4,3,5)   #(r.IT,r.XN,r.L,(r.KW//2+1),r.CO_PRL,c.ROWS)\n","        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,(r.KW//2+1),r.CO_PRL,c.ROWS)\n","        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,(r.KW//2+1)*r.CO_PRL,c.ROWS)\n","        \n","        y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:] = y_w_last\n","\n","        y = y.reshape(r.IT,r.XN,r.L,r.XW,r.CO_PRL,c.ROWS)\n","        y = y.transpose(1,2,5,3,0,4)\n","        y = y.reshape((r.XN, r.L*c.ROWS, r.XW, r.CO_PAD))\n","        y = y[:,:r.XH,:,:r.CO]\n","\n","        return y\n","\n","class BModel(tf.keras.Model):\n","    def __init__(self,\n","                 **kwargs):\n","        super(BModel, self).__init__()\n","\n","\n","'''\n","Build Model\n","'''\n","\n","x = x_in = Input(input_shape, name='input')\n","x = QActivation(q_0)(x)\n","\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':64, 'kernel_size':(7,7), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_0},\n","    pool= {'type':'max', 'size':(3,3), 'strides':(1,1), 'padding':'same', 'act_str': q_0}\n","    )(x)\n","\n","# block 0\n","x = Bundle(\n","    core= {'type':'conv', 'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_0}\n","    )(x)\n","\n","\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_1},\n","    add= {'act_str': a_0}\n","    )(x, x1)\n","\n","x = Bundle(\n","    core= {'type':'conv', 'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_0}\n","    )(x)\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_1}, \n","    add= {'act_str': a_1}\n","    )(x, x1)\n","\n","# block 1\n","x1 = Bundle(\n","    core= {'type':'conv', 'filters':128, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_1}\n","    )(x1)\n","x1 = Bundle(\n","    core= {'type':'conv', 'filters':128, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_2}\n","    )(x1)\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':128, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_2},\n","    add={'act_str':a_2}\n","    )(x, x1)\n","\n","x = Bundle(\n","    core= {'type':'conv', 'filters':128, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_1}\n","    )(x)\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':128, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_2},\n","    add={'act_str':a_2}\n","    )(x, x1)\n","\n","#block 2\n","x1 = Bundle(\n","    core= {'type':'conv', 'filters':256, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_1}\n","    )(x1)\n","x1 = Bundle(\n","    core= {'type':'conv', 'filters':256, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_2}\n","    )(x1)\n","x = x1 = Bundle(\n","    core = {'type':'conv', 'filters':256, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_3},\n","    add= {'act_str':a_3}\n","    )(x, x1)\n","\n","x = Bundle(\n","    core = {'type':'conv', 'filters':256, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_2}\n","    )(x)\n","x = x1 = Bundle(\n","    core = {'type':'conv', 'filters':256, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_3},\n","    add= {'act_str':a_3}\n","    )(x, x1)\n","\n","#block 3\n","x1 = Bundle(\n","    core={'type':'conv', 'filters':512, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_2}\n","    )(x1)\n","x1 = Bundle(\n","    core= {'type':'conv', 'filters':512, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_3}\n","    )(x1)\n","x = x1 = Bundle(\n","    core= {'type':'conv', 'filters':512, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_3}, \n","    add= {'act_str':a_3}\n","    )(x, x1)\n","\n","x = Bundle(\n","    core= {'type':'conv', 'filters':512, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':a_2}\n","    )(x)\n","x = Bundle(\n","    core= {'type':'conv', 'filters':512, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS, 'act_str':q_3}, \n","    add= {'act_str':a_3},\n","    pool= {'type':'avg', 'size':(2,2), 'strides':(2,2), 'padding':'valid', 'act_str': q_3},\n","    flatten=True\n","    )(x, x1)\n","\n","x = Bundle(\n","    core= {'type':'dense', 'units':10, 'kernel_quantizer':q_2, 'bias_quantizer':q_2, 'use_bias':USE_BIAS, 'act_str': q_3}, \n","    softmax=True)(x)\n","\n","model = Model(inputs=x_in, outputs=x)\n","print(model.summary(expand_nested=True))\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"qSxuQVKda_OC","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning rate:  0.001\n"]}],"source":["def lr_schedule(epoch):\n","    \"\"\"\n","  Bundles_pre_trainearning Rate Schedule\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","    # Arguments\n","        epoch (int): The number of epochs\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    # initial_lr = 1e-4\n","    # lr_decay = 0.99\n","    # lr = initial_lr * (lr_decay ** epoch)\n","    lr = 1e-3 # default 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 150:\n","        lr *= 1e-2\n","    elif epoch > 100:\n","        lr *= 1e-1\n","    elif epoch > 50:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","preamble = ''\n","model_file_path = preamble+'resnet18.h5'\n","checkpoint = ModelCheckpoint(filepath=model_file_path,\n","                                     monitor='val_acc',\n","                                     verbose=1,\n","                                     save_best_only=True)\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                                       cooldown=0,\n","                                       patience=5,\n","                                       min_lr=0.5e-6)\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","NB_EPOCH = 200\n","BATCH_SIZE = 256\n","VERBOSE = 1\n","VALIDATION_SPLIT = 0.1\n","RELU_NEG_SLOPE = 0.125\n","\n","model.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=lr_schedule(0)), metrics=['acc'])\n","\n","# model.fit(x_train, y_train,\n","#                     batch_size=BATCH_SIZE,\n","#                     epochs=NB_EPOCH,\n","#                     validation_data=(x_test, y_test),\n","#                     shuffle=True,\n","#                     callbacks=callbacks)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LGQRt23Na_OD"},"outputs":[],"source":["XN = 4\n","x = np.random.randn(XN, *model.input.shape[1:])\n","x = np.clip(x, -1.0, 1.0)\n","\n","inp_act_model = Model(inputs=model.input, outputs=model.layers[1].output)\n","inp ={ 'tensor': inp_act_model(x, training=False), 'bits':8, 'frac':7}\n","inp['int'] = inp['tensor'].numpy() * 2**inp['frac']\n","\n","y = model(x)\n","\n","model.layers[2].process(inp)\n","for layer in model.layers[3:]:\n","    layer.process()\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------0-----------------------\n","weights initial (KH, KW, CI, CO) = (7, 7, 3, 64)\n","KH=7, KW=7, CI=3, CO=64, CO_PRL=3, EG=3, IT=22, 66\n","input initial (XN, XH, XW, CI)= (4, 32, 32, 3)\n","output initial (4, 16, 16, 64)\n","{'w_shape': (7, 7, 3, 64), 'x_shape': (4, 32, 32, 3), 'y_shape': (4, 16, 16, 64), 'SW': 1, 'SH': 1, 'KH': 7, 'KW': 7, 'CI': 3, 'CO': 64, 'CO_PRL': 3, 'EG': 3, 'IT': 22, 'CO_PAD': 66, 'XN': 4, 'XH': 32, 'XW': 32, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 4, 'XH_PAD': 32, 'BRAM_WEIGHTS_ADDR_MAX': 21}\n","Runtime(w_shape=(7, 7, 3, 64), x_shape=(4, 32, 32, 3), y_shape=(4, 16, 16, 64), SW=1, SH=1, KH=7, KW=7, CI=3, CO=64, CO_PRL=3, EG=3, IT=22, CO_PAD=66, XN=4, XH=32, XW=32, SH_OUT=2, SW_OUT=2, LH=8, L=4, XH_PAD=32, BRAM_WEIGHTS_ADDR_MAX=21, w_config='0b00000000000000000000000000000000101010011111111100000000010011', w_config_words=array([[ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0],\n","       [ 19, -64, 127,  42,   0,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000111111100000000010011', x_config_words=ListWrapper([19, 192, 31, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 2.32%\n","        x_sparsity   : 0.33%\n","\n","        both_zero    : 0.01%\n","        only_one_zero: 2.64%\n","        neither_zero : 97.36%\n","        zero_result  : 2.64%\n","        \n","(7, 7, 3, 66) (7, 7, 3, 22, 3)\n","-----------------1-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 64, 64)\n","KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, 64\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 16, 16, 64)\n","{'w_shape': (3, 3, 64, 64), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 16, 16, 64), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 64, 'CO': 64, 'CO_PRL': 8, 'EG': 8, 'IT': 8, 'CO_PAD': 64, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 192}\n","Runtime(w_shape=(3, 3, 64, 64), x_shape=(4, 16, 16, 64), y_shape=(4, 16, 16, 64), SW=1, SH=1, KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, CO_PAD=64, XN=4, XH=16, XW=16, SH_OUT=1, SW_OUT=1, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=192, w_config='0b00000000000000000000000000000110000000011010111100000111111001', w_config_words=array([[  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111001', x_config_words=ListWrapper([249, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 4.62%\n","        x_sparsity   : 0.11%\n","\n","        both_zero    : 0.01%\n","        only_one_zero: 4.72%\n","        neither_zero : 95.28%\n","        zero_result  : 4.72%\n","        \n","(3, 3, 64, 64) (3, 3, 64, 8, 8)\n","-----------------2-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 64, 64)\n","KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, 64\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 16, 16, 64)\n","{'w_shape': (3, 3, 64, 64), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 16, 16, 64), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 64, 'CO': 64, 'CO_PRL': 8, 'EG': 8, 'IT': 8, 'CO_PAD': 64, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 192}\n","Runtime(w_shape=(3, 3, 64, 64), x_shape=(4, 16, 16, 64), y_shape=(4, 16, 16, 64), SW=1, SH=1, KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, CO_PAD=64, XN=4, XH=16, XW=16, SH_OUT=1, SW_OUT=1, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=192, w_config='0b00000000000000000000000000000110000000011010111100000111111001', w_config_words=array([[  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111001', x_config_words=ListWrapper([249, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 4.96%\n","        x_sparsity   : 1.03%\n","\n","        both_zero    : 0.05%\n","        only_one_zero: 5.89%\n","        neither_zero : 94.06%\n","        zero_result  : 5.94%\n","        \n","(3, 3, 64, 64) (3, 3, 64, 8, 8)\n","-----------------3-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 64, 64)\n","KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, 64\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 16, 16, 64)\n","{'w_shape': (3, 3, 64, 64), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 16, 16, 64), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 64, 'CO': 64, 'CO_PRL': 8, 'EG': 8, 'IT': 8, 'CO_PAD': 64, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 192}\n","Runtime(w_shape=(3, 3, 64, 64), x_shape=(4, 16, 16, 64), y_shape=(4, 16, 16, 64), SW=1, SH=1, KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, CO_PAD=64, XN=4, XH=16, XW=16, SH_OUT=1, SW_OUT=1, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=192, w_config='0b00000000000000000000000000000110000000011010111100000111111001', w_config_words=array([[  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111001', x_config_words=ListWrapper([249, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 4.84%\n","        x_sparsity   : 0.92%\n","\n","        both_zero    : 0.04%\n","        only_one_zero: 5.68%\n","        neither_zero : 94.28%\n","        zero_result  : 5.72%\n","        \n","(3, 3, 64, 64) (3, 3, 64, 8, 8)\n","-----------------4-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 64, 64)\n","KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, 64\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 16, 16, 64)\n","{'w_shape': (3, 3, 64, 64), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 16, 16, 64), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 64, 'CO': 64, 'CO_PRL': 8, 'EG': 8, 'IT': 8, 'CO_PAD': 64, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 192}\n","Runtime(w_shape=(3, 3, 64, 64), x_shape=(4, 16, 16, 64), y_shape=(4, 16, 16, 64), SW=1, SH=1, KH=3, KW=3, CI=64, CO=64, CO_PRL=8, EG=8, IT=8, CO_PAD=64, XN=4, XH=16, XW=16, SH_OUT=1, SW_OUT=1, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=192, w_config='0b00000000000000000000000000000110000000011010111100000111111001', w_config_words=array([[  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111001', x_config_words=ListWrapper([249, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 4.80%\n","        x_sparsity   : 1.10%\n","\n","        both_zero    : 0.05%\n","        only_one_zero: 5.79%\n","        neither_zero : 94.15%\n","        zero_result  : 5.85%\n","        \n","(3, 3, 64, 64) (3, 3, 64, 8, 8)\n","-----------------5-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 64, 128)\n","KH=3, KW=3, CI=64, CO=128, CO_PRL=8, EG=8, IT=16, 128\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 8, 8, 128)\n","{'w_shape': (3, 3, 64, 128), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 8, 8, 128), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 64, 'CO': 128, 'CO_PRL': 8, 'EG': 8, 'IT': 16, 'CO_PAD': 128, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 192}\n","Runtime(w_shape=(3, 3, 64, 128), x_shape=(4, 16, 16, 64), y_shape=(4, 8, 8, 128), SW=1, SH=1, KH=3, KW=3, CI=64, CO=128, CO_PRL=8, EG=8, IT=16, CO_PAD=128, XN=4, XH=16, XW=16, SH_OUT=2, SW_OUT=2, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=192, w_config='0b00000000000000000000000000000110000000011010111100000111111001', w_config_words=array([[  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0],\n","       [  -7,  -63,  107, -128,    1,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111001', x_config_words=ListWrapper([249, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 5.03%\n","        x_sparsity   : 2.81%\n","\n","        both_zero    : 0.14%\n","        only_one_zero: 7.56%\n","        neither_zero : 92.30%\n","        zero_result  : 7.70%\n","        \n","(3, 3, 64, 128) (3, 3, 64, 16, 8)\n","-----------------6-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 128, 128)\n","KH=3, KW=3, CI=128, CO=128, CO_PRL=8, EG=8, IT=16, 128\n","input initial (XN, XH, XW, CI)= (4, 8, 8, 128)\n","output initial (4, 8, 8, 128)\n","{'w_shape': (3, 3, 128, 128), 'x_shape': (4, 8, 8, 128), 'y_shape': (4, 8, 8, 128), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 128, 'CO': 128, 'CO_PRL': 8, 'EG': 8, 'IT': 16, 'CO_PAD': 128, 'XN': 4, 'XH': 8, 'XW': 8, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 384}\n","Runtime(w_shape=(3, 3, 128, 128), x_shape=(4, 8, 8, 128), y_shape=(4, 8, 8, 128), SW=1, SH=1, KH=3, KW=3, CI=128, CO=128, CO_PRL=8, EG=8, IT=16, CO_PAD=128, XN=4, XH=8, XW=8, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=384, w_config='0b00000000000000000000000000001100000000011000011100001111111001', w_config_words=array([[ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000011100001111111001', x_config_words=ListWrapper([249, 195, 1, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 6.93%\n","        x_sparsity   : 2.23%\n","\n","        both_zero    : 0.15%\n","        only_one_zero: 8.84%\n","        neither_zero : 91.00%\n","        zero_result  : 9.00%\n","        \n","(3, 3, 128, 128) (3, 3, 128, 16, 8)\n","-----------------5-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 64, 128)\n","KH=1, KW=1, CI=64, CO=128, CO_PRL=24, EG=24, IT=6, 144\n","input initial (XN, XH, XW, CI)= (4, 16, 16, 64)\n","output initial (4, 8, 8, 128)\n","{'w_shape': (1, 1, 64, 128), 'x_shape': (4, 16, 16, 64), 'y_shape': (4, 8, 8, 128), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 64, 'CO': 128, 'CO_PRL': 24, 'EG': 24, 'IT': 6, 'CO_PAD': 144, 'XN': 4, 'XH': 16, 'XW': 16, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 2, 'XH_PAD': 16, 'BRAM_WEIGHTS_ADDR_MAX': 64}\n","Runtime(w_shape=(1, 1, 64, 128), x_shape=(4, 16, 16, 64), y_shape=(4, 8, 8, 128), SW=1, SH=1, KH=1, KW=1, CI=64, CO=128, CO_PRL=24, EG=24, IT=6, CO_PAD=144, XN=4, XH=16, XW=16, SH_OUT=2, SW_OUT=2, LH=8, L=2, XH_PAD=16, BRAM_WEIGHTS_ADDR_MAX=64, w_config='0b00000000000000000000000000000010000000011010111100000111111000', w_config_words=array([[  -8,  -63,  107, -128,    0,    0,    0,    0],\n","       [  -8,  -63,  107, -128,    0,    0,    0,    0],\n","       [  -8,  -63,  107, -128,    0,    0,    0,    0],\n","       [  -8,  -63,  107, -128,    0,    0,    0,    0],\n","       [  -8,  -63,  107, -128,    0,    0,    0,    0],\n","       [  -8,  -63,  107, -128,    0,    0,    0,    0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000010111100000111111000', x_config_words=ListWrapper([248, 193, 11, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 1.72%\n","        x_sparsity   : 2.81%\n","\n","        both_zero    : 0.05%\n","        only_one_zero: 4.43%\n","        neither_zero : 95.52%\n","        zero_result  : 4.48%\n","        \n","(1, 1, 64, 144) (1, 1, 64, 6, 24)\n","-----------------6-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 128, 128)\n","KH=3, KW=3, CI=128, CO=128, CO_PRL=8, EG=8, IT=16, 128\n","input initial (XN, XH, XW, CI)= (4, 8, 8, 128)\n","output initial (4, 8, 8, 128)\n","{'w_shape': (3, 3, 128, 128), 'x_shape': (4, 8, 8, 128), 'y_shape': (4, 8, 8, 128), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 128, 'CO': 128, 'CO_PRL': 8, 'EG': 8, 'IT': 16, 'CO_PAD': 128, 'XN': 4, 'XH': 8, 'XW': 8, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 384}\n","Runtime(w_shape=(3, 3, 128, 128), x_shape=(4, 8, 8, 128), y_shape=(4, 8, 8, 128), SW=1, SH=1, KH=3, KW=3, CI=128, CO=128, CO_PRL=8, EG=8, IT=16, CO_PAD=128, XN=4, XH=8, XW=8, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=384, w_config='0b00000000000000000000000000001100000000011000011100001111111001', w_config_words=array([[ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000011100001111111001', x_config_words=ListWrapper([249, 195, 1, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 6.95%\n","        x_sparsity   : 3.79%\n","\n","        both_zero    : 0.26%\n","        only_one_zero: 10.21%\n","        neither_zero : 89.52%\n","        zero_result  : 10.48%\n","        \n","(3, 3, 128, 128) (3, 3, 128, 16, 8)\n","-----------------7-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 128, 128)\n","KH=1, KW=1, CI=128, CO=128, CO_PRL=24, EG=24, IT=6, 144\n","input initial (XN, XH, XW, CI)= (4, 8, 8, 128)\n","output initial (4, 8, 8, 128)\n","{'w_shape': (1, 1, 128, 128), 'x_shape': (4, 8, 8, 128), 'y_shape': (4, 8, 8, 128), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 128, 'CO': 128, 'CO_PRL': 24, 'EG': 24, 'IT': 6, 'CO_PAD': 144, 'XN': 4, 'XH': 8, 'XW': 8, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 128}\n","Runtime(w_shape=(1, 1, 128, 128), x_shape=(4, 8, 8, 128), y_shape=(4, 8, 8, 128), SW=1, SH=1, KH=1, KW=1, CI=128, CO=128, CO_PRL=24, EG=24, IT=6, CO_PAD=144, XN=4, XH=8, XW=8, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=128, w_config='0b00000000000000000000000000000100000000011000011100001111111000', w_config_words=array([[ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000011100001111111000', x_config_words=ListWrapper([248, 195, 1, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 2.11%\n","        x_sparsity   : 1.72%\n","\n","        both_zero    : 0.04%\n","        only_one_zero: 3.75%\n","        neither_zero : 96.21%\n","        zero_result  : 3.79%\n","        \n","(1, 1, 128, 144) (1, 1, 128, 6, 24)\n","-----------------8-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 128, 256)\n","KH=3, KW=3, CI=128, CO=256, CO_PRL=8, EG=8, IT=32, 256\n","input initial (XN, XH, XW, CI)= (4, 8, 8, 128)\n","output initial (4, 4, 4, 256)\n","{'w_shape': (3, 3, 128, 256), 'x_shape': (4, 8, 8, 128), 'y_shape': (4, 4, 4, 256), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 128, 'CO': 256, 'CO_PRL': 8, 'EG': 8, 'IT': 32, 'CO_PAD': 256, 'XN': 4, 'XH': 8, 'XW': 8, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 384}\n","Runtime(w_shape=(3, 3, 128, 256), x_shape=(4, 8, 8, 128), y_shape=(4, 4, 4, 256), SW=1, SH=1, KH=3, KW=3, CI=128, CO=256, CO_PRL=8, EG=8, IT=32, CO_PAD=256, XN=4, XH=8, XW=8, SH_OUT=2, SW_OUT=2, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=384, w_config='0b00000000000000000000000000001100000000011000011100001111111001', w_config_words=array([[ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0],\n","       [ -7, -61,  97,   0,   3,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000011100001111111001', x_config_words=ListWrapper([249, 195, 1, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 6.99%\n","        x_sparsity   : 4.06%\n","\n","        both_zero    : 0.28%\n","        only_one_zero: 10.47%\n","        neither_zero : 89.24%\n","        zero_result  : 10.76%\n","        \n","(3, 3, 128, 256) (3, 3, 128, 32, 8)\n","-----------------9-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 256, 256)\n","KH=3, KW=3, CI=256, CO=256, CO_PRL=8, EG=8, IT=32, 256\n","input initial (XN, XH, XW, CI)= (4, 4, 4, 256)\n","output initial (4, 4, 4, 256)\n","{'w_shape': (3, 3, 256, 256), 'x_shape': (4, 4, 4, 256), 'y_shape': (4, 4, 4, 256), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 256, 'CO': 256, 'CO_PRL': 8, 'EG': 8, 'IT': 32, 'CO_PAD': 256, 'XN': 4, 'XH': 4, 'XW': 4, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 768}\n","Runtime(w_shape=(3, 3, 256, 256), x_shape=(4, 4, 4, 256), y_shape=(4, 4, 4, 256), SW=1, SH=1, KH=3, KW=3, CI=256, CO=256, CO_PRL=8, EG=8, IT=32, CO_PAD=256, XN=4, XH=4, XW=4, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=768, w_config='0b00000000000000000000000000011000000000011000001100011111111001', w_config_words=array([[ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000001100011111111001', x_config_words=ListWrapper([249, 199, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 9.80%\n","        x_sparsity   : 1.14%\n","\n","        both_zero    : 0.11%\n","        only_one_zero: 10.72%\n","        neither_zero : 89.17%\n","        zero_result  : 10.83%\n","        \n","(3, 3, 256, 256) (3, 3, 256, 32, 8)\n","-----------------8-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 128, 256)\n","KH=1, KW=1, CI=128, CO=256, CO_PRL=24, EG=24, IT=11, 264\n","input initial (XN, XH, XW, CI)= (4, 8, 8, 128)\n","output initial (4, 4, 4, 256)\n","{'w_shape': (1, 1, 128, 256), 'x_shape': (4, 8, 8, 128), 'y_shape': (4, 4, 4, 256), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 128, 'CO': 256, 'CO_PRL': 24, 'EG': 24, 'IT': 11, 'CO_PAD': 264, 'XN': 4, 'XH': 8, 'XW': 8, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 128}\n","Runtime(w_shape=(1, 1, 128, 256), x_shape=(4, 8, 8, 128), y_shape=(4, 4, 4, 256), SW=1, SH=1, KH=1, KW=1, CI=128, CO=256, CO_PRL=24, EG=24, IT=11, CO_PAD=264, XN=4, XH=8, XW=8, SH_OUT=2, SW_OUT=2, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=128, w_config='0b00000000000000000000000000000100000000011000011100001111111000', w_config_words=array([[ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0],\n","       [ -8, -61,  97,   0,   1,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000011100001111111000', x_config_words=ListWrapper([248, 195, 1, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 2.37%\n","        x_sparsity   : 4.06%\n","\n","        both_zero    : 0.10%\n","        only_one_zero: 6.24%\n","        neither_zero : 93.67%\n","        zero_result  : 6.33%\n","        \n","(1, 1, 128, 264) (1, 1, 128, 11, 24)\n","-----------------9-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 256, 256)\n","KH=3, KW=3, CI=256, CO=256, CO_PRL=8, EG=8, IT=32, 256\n","input initial (XN, XH, XW, CI)= (4, 4, 4, 256)\n","output initial (4, 4, 4, 256)\n","{'w_shape': (3, 3, 256, 256), 'x_shape': (4, 4, 4, 256), 'y_shape': (4, 4, 4, 256), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 256, 'CO': 256, 'CO_PRL': 8, 'EG': 8, 'IT': 32, 'CO_PAD': 256, 'XN': 4, 'XH': 4, 'XW': 4, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 768}\n","Runtime(w_shape=(3, 3, 256, 256), x_shape=(4, 4, 4, 256), y_shape=(4, 4, 4, 256), SW=1, SH=1, KH=3, KW=3, CI=256, CO=256, CO_PRL=8, EG=8, IT=32, CO_PAD=256, XN=4, XH=4, XW=4, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=768, w_config='0b00000000000000000000000000011000000000011000001100011111111001', w_config_words=array([[ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000001100011111111001', x_config_words=ListWrapper([249, 199, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 9.78%\n","        x_sparsity   : 5.37%\n","\n","        both_zero    : 0.53%\n","        only_one_zero: 14.10%\n","        neither_zero : 85.37%\n","        zero_result  : 14.63%\n","        \n","(3, 3, 256, 256) (3, 3, 256, 32, 8)\n","-----------------10-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 256, 256)\n","KH=1, KW=1, CI=256, CO=256, CO_PRL=24, EG=24, IT=11, 264\n","input initial (XN, XH, XW, CI)= (4, 4, 4, 256)\n","output initial (4, 4, 4, 256)\n","{'w_shape': (1, 1, 256, 256), 'x_shape': (4, 4, 4, 256), 'y_shape': (4, 4, 4, 256), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 256, 'CO': 256, 'CO_PRL': 24, 'EG': 24, 'IT': 11, 'CO_PAD': 264, 'XN': 4, 'XH': 4, 'XW': 4, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 256}\n","Runtime(w_shape=(1, 1, 256, 256), x_shape=(4, 4, 4, 256), y_shape=(4, 4, 4, 256), SW=1, SH=1, KH=1, KW=1, CI=256, CO=256, CO_PRL=24, EG=24, IT=11, CO_PAD=264, XN=4, XH=4, XW=4, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=256, w_config='0b00000000000000000000000000001000000000011000001100011111111000', w_config_words=array([[ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000001100011111111000', x_config_words=ListWrapper([248, 199, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 3.29%\n","        x_sparsity   : 2.83%\n","\n","        both_zero    : 0.09%\n","        only_one_zero: 5.93%\n","        neither_zero : 93.97%\n","        zero_result  : 6.03%\n","        \n","(1, 1, 256, 264) (1, 1, 256, 11, 24)\n","-----------------11-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 256, 512)\n","KH=3, KW=3, CI=256, CO=512, CO_PRL=8, EG=8, IT=64, 512\n","input initial (XN, XH, XW, CI)= (4, 4, 4, 256)\n","output initial (4, 2, 2, 512)\n","{'w_shape': (3, 3, 256, 512), 'x_shape': (4, 4, 4, 256), 'y_shape': (4, 2, 2, 512), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 256, 'CO': 512, 'CO_PRL': 8, 'EG': 8, 'IT': 64, 'CO_PAD': 512, 'XN': 4, 'XH': 4, 'XW': 4, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 768}\n","Runtime(w_shape=(3, 3, 256, 512), x_shape=(4, 4, 4, 256), y_shape=(4, 2, 2, 512), SW=1, SH=1, KH=3, KW=3, CI=256, CO=512, CO_PRL=8, EG=8, IT=64, CO_PAD=512, XN=4, XH=4, XW=4, SH_OUT=2, SW_OUT=2, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=768, w_config='0b00000000000000000000000000011000000000011000001100011111111001', w_config_words=array([[ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0],\n","       [ -7, -57,  96,   0,   6,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000001100011111111001', x_config_words=ListWrapper([249, 199, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 9.74%\n","        x_sparsity   : 5.07%\n","\n","        both_zero    : 0.49%\n","        only_one_zero: 13.82%\n","        neither_zero : 85.69%\n","        zero_result  : 14.31%\n","        \n","(3, 3, 256, 512) (3, 3, 256, 64, 8)\n","-----------------12-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 512, 512)\n","KH=3, KW=3, CI=512, CO=512, CO_PRL=8, EG=8, IT=64, 512\n","input initial (XN, XH, XW, CI)= (4, 2, 2, 512)\n","output initial (4, 2, 2, 512)\n","{'w_shape': (3, 3, 512, 512), 'x_shape': (4, 2, 2, 512), 'y_shape': (4, 2, 2, 512), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 512, 'CO': 512, 'CO_PRL': 8, 'EG': 8, 'IT': 64, 'CO_PAD': 512, 'XN': 4, 'XH': 2, 'XW': 2, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 1536}\n","Runtime(w_shape=(3, 3, 512, 512), x_shape=(4, 2, 2, 512), y_shape=(4, 2, 2, 512), SW=1, SH=1, KH=3, KW=3, CI=512, CO=512, CO_PRL=8, EG=8, IT=64, CO_PAD=512, XN=4, XH=2, XW=2, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=1536, w_config='0b00000000000000000000000000110000000000011000000100111111111001', w_config_words=array([[-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000000100111111111001', x_config_words=ListWrapper([249, 79, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 13.75%\n","        x_sparsity   : 2.27%\n","\n","        both_zero    : 0.31%\n","        only_one_zero: 15.39%\n","        neither_zero : 84.30%\n","        zero_result  : 15.70%\n","        \n","(3, 3, 512, 512) (3, 3, 512, 64, 8)\n","-----------------11-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 256, 512)\n","KH=1, KW=1, CI=256, CO=512, CO_PRL=24, EG=24, IT=22, 528\n","input initial (XN, XH, XW, CI)= (4, 4, 4, 256)\n","output initial (4, 2, 2, 512)\n","{'w_shape': (1, 1, 256, 512), 'x_shape': (4, 4, 4, 256), 'y_shape': (4, 2, 2, 512), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 256, 'CO': 512, 'CO_PRL': 24, 'EG': 24, 'IT': 22, 'CO_PAD': 528, 'XN': 4, 'XH': 4, 'XW': 4, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 256}\n","Runtime(w_shape=(1, 1, 256, 512), x_shape=(4, 4, 4, 256), y_shape=(4, 2, 2, 512), SW=1, SH=1, KH=1, KW=1, CI=256, CO=512, CO_PRL=24, EG=24, IT=22, CO_PAD=528, XN=4, XH=4, XW=4, SH_OUT=2, SW_OUT=2, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=256, w_config='0b00000000000000000000000000001000000000011000001100011111111000', w_config_words=array([[ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0],\n","       [ -8, -57,  96,   0,   2,   0,   0,   0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000001100011111111000', x_config_words=ListWrapper([248, 199, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 3.28%\n","        x_sparsity   : 5.07%\n","\n","        both_zero    : 0.17%\n","        only_one_zero: 8.02%\n","        neither_zero : 91.82%\n","        zero_result  : 8.18%\n","        \n","(1, 1, 256, 528) (1, 1, 256, 22, 24)\n","-----------------12-----------------------\n","weights initial (KH, KW, CI, CO) = (3, 3, 512, 512)\n","KH=3, KW=3, CI=512, CO=512, CO_PRL=8, EG=8, IT=64, 512\n","input initial (XN, XH, XW, CI)= (4, 2, 2, 512)\n","output initial (4, 2, 2, 512)\n","{'w_shape': (3, 3, 512, 512), 'x_shape': (4, 2, 2, 512), 'y_shape': (4, 2, 2, 512), 'SW': 1, 'SH': 1, 'KH': 3, 'KW': 3, 'CI': 512, 'CO': 512, 'CO_PRL': 8, 'EG': 8, 'IT': 64, 'CO_PAD': 512, 'XN': 4, 'XH': 2, 'XW': 2, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 1536}\n","Runtime(w_shape=(3, 3, 512, 512), x_shape=(4, 2, 2, 512), y_shape=(4, 2, 2, 512), SW=1, SH=1, KH=3, KW=3, CI=512, CO=512, CO_PRL=8, EG=8, IT=64, CO_PAD=512, XN=4, XH=2, XW=2, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=1536, w_config='0b00000000000000000000000000110000000000011000000100111111111001', w_config_words=array([[-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0],\n","       [-7, 79, 96,  0, 12,  0,  0,  0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000000100111111111001', x_config_words=ListWrapper([249, 79, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 13.69%\n","        x_sparsity   : 3.23%\n","\n","        both_zero    : 0.44%\n","        only_one_zero: 16.04%\n","        neither_zero : 83.51%\n","        zero_result  : 16.49%\n","        \n","(3, 3, 512, 512) (3, 3, 512, 64, 8)\n","-----------------13-----------------------\n","weights initial (KH, KW, CI, CO) = (1, 1, 512, 512)\n","KH=1, KW=1, CI=512, CO=512, CO_PRL=24, EG=24, IT=22, 528\n","input initial (XN, XH, XW, CI)= (4, 2, 2, 512)\n","output initial (4, 1, 1, 512)\n","{'w_shape': (1, 1, 512, 512), 'x_shape': (4, 2, 2, 512), 'y_shape': (4, 1, 1, 512), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 512, 'CO': 512, 'CO_PRL': 24, 'EG': 24, 'IT': 22, 'CO_PAD': 528, 'XN': 4, 'XH': 2, 'XW': 2, 'SH_OUT': 2, 'SW_OUT': 2, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 512}\n","Runtime(w_shape=(1, 1, 512, 512), x_shape=(4, 2, 2, 512), y_shape=(4, 1, 1, 512), SW=1, SH=1, KH=1, KW=1, CI=512, CO=512, CO_PRL=24, EG=24, IT=22, CO_PAD=528, XN=4, XH=2, XW=2, SH_OUT=2, SW_OUT=2, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=512, w_config='0b00000000000000000000000000010000000000011000000100111111111000', w_config_words=array([[-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0],\n","       [-8, 79, 96,  0,  4,  0,  0,  0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000000100111111111000', x_config_words=ListWrapper([248, 79, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 4.55%\n","        x_sparsity   : 2.04%\n","\n","        both_zero    : 0.09%\n","        only_one_zero: 6.40%\n","        neither_zero : 93.51%\n","        zero_result  : 6.49%\n","        \n","(1, 1, 512, 528) (1, 1, 512, 22, 24)\n","-----------------14-----------------------\n","Conv -> Dense Reshape\n","weights initial (KH, KW, CI, CO) = (1, 1, 512, 10)\n","KH=1, KW=1, CI=512, CO=10, CO_PRL=24, EG=24, IT=1, 24\n","input initial (XN, XH, XW, CI)= (4, 1, 1, 512)\n","output initial (4, 1, 1, 10)\n","{'w_shape': (1, 1, 512, 10), 'x_shape': (4, 1, 1, 512), 'y_shape': (4, 1, 1, 10), 'SW': 1, 'SH': 1, 'KH': 1, 'KW': 1, 'CI': 512, 'CO': 10, 'CO_PRL': 24, 'EG': 24, 'IT': 1, 'CO_PAD': 24, 'XN': 4, 'XH': 1, 'XW': 1, 'SH_OUT': 1, 'SW_OUT': 1, 'LH': 8, 'L': 1, 'XH_PAD': 8, 'BRAM_WEIGHTS_ADDR_MAX': 512}\n","Runtime(w_shape=(1, 1, 512, 10), x_shape=(4, 1, 1, 512), y_shape=(4, 1, 1, 10), SW=1, SH=1, KH=1, KW=1, CI=512, CO=10, CO_PRL=24, EG=24, IT=1, CO_PAD=24, XN=4, XH=1, XW=1, SH_OUT=1, SW_OUT=1, LH=8, L=1, XH_PAD=8, BRAM_WEIGHTS_ADDR_MAX=512, w_config='0b00000000000000000000000000010000000000011000000000111111111000', w_config_words=array([[-8, 15, 96,  0,  4,  0,  0,  0]], dtype=int8), x_config='0b00000000000000000000000000000000000000000000000000111111111000', x_config_words=ListWrapper([248, 15, 0, 0, 0, 0, 0, 0]))\n","\n","        w_sparsity   : 17.89%\n","        x_sparsity   : 1.56%\n","\n","        both_zero    : 0.28%\n","        only_one_zero: 18.89%\n","        neither_zero : 80.83%\n","        zero_result  : 19.17%\n","        \n","(1, 1, 512, 24) (1, 1, 512, 1, 24)\n"]}],"source":["with open('../compile.pickle', 'rb') as f:\n","    compile_d = pickle.load(f)\n","    c = namedtuple('Compile', compile_d)(**compile_d)\n","\n","bundles = model.layers[2:]\n","for bundle in bundles:\n","    print(f'-----------------{bundle.idx}-----------------------')\n","    bundle.export(c)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import pickle\n","pickle.dump(bundles, open(\"../models/bundles.pickle\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
