{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ABy3xAE8uW__"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n","c:\\ProgramData\\Miniconda3\\envs\\qkeras\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["from qkeras import *\n","from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Softmax, Add, ZeroPadding2D, MaxPooling2D\n","import numpy as np\n","from collections import namedtuple\n","import pickle\n","import math\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.utils import to_categorical\n","from qkeras.utils import model_save_quantized_weights"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vG3iXUBnuXAB"},"outputs":[],"source":["def load_data(num_classes=10, subtract_pixel_mean=True):\n","    \"\"\"\n","    Load CIFAR10 data and normalize\n","    \"\"\"\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","    # input image dimensions.\n","    input_shape = x_train.shape[1:]\n","\n","    # normalize data.\n","    x_train = x_train.astype('float32') / 128.0 - 1.0\n","    x_test = x_test.astype('float32') / 128.0 - 1.0\n","\n","    # if subtract pixel mean is enabled\n","    if subtract_pixel_mean:\n","        x_train_mean = np.mean(x_train, axis=0)\n","        x_train -= x_train_mean\n","        x_test -= x_train_mean\n","\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","    print('y_train shape:', y_train.shape)\n","\n","    # convert class vectors to binary class matrices,\n","    # i.e., one hot encodings\n","    y_train = to_categorical(y_train, num_classes)\n","    y_test = to_categorical(y_test, num_classes)\n","\n","    return x_train, y_train, x_test, y_test\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtMyMdKTuXAC","outputId":"24e6c7d8-b3bc-4b07-a12f-3d3955194087"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n","y_train shape: (50000, 1)\n"]}],"source":["x_train, y_train, x_test, y_test = load_data(10, False)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FtdLlJbzuXAC"},"outputs":[],"source":["input_shape = x_train.shape[1:-1] + (3,)\n","np.random.seed(1)\n","\n","a_0 = 'quantized_relu(8,0,negative_slope=0.125)'\n","a_1 = 'quantized_relu(8,1,negative_slope=0.125)'\n","a_2 = 'quantized_relu(8,2,negative_slope=0.125)'\n","a_3 = 'quantized_relu(8,3,negative_slope=0.125)'\n","\n","q_0 = 'quantized_bits(8,0,False,True,1)'\n","q_1 = 'quantized_bits(8,1,False,True,1)'\n","q_2 = 'quantized_bits(8,2,False,True,1)'\n","q_3 = 'quantized_bits(8,3,False,True,1)'\n","\n","q_t = 'quantized_bits(8,0,False,True,1)'\n","\n","np.random.seed(42)\n","#preamble = './drive/MyDrive/resnet/'\n","preamble = ''\n","USE_BIAS = True"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qgoMo_ima_OB"},"outputs":[],"source":["class Bundle(tf.keras.Model):\n","    def __init__(self, serial,       # int, specify the sequence of the bundle, start from 0, seek to automate!\n","                 core_type,          # str, Mandatory: must be conv or dense\n","                 core_params,        # dict, Mandaroty: parameters for conv/dense layer\n","                 act_core,           # str, Mandatory, can be quantization or relu\n","\n","                 act_add=None,       # str, Mandatory if x1 is not None in call(), else ignored\n","\n","                 pool_type=None,     # str, Optional: can only be max or avg\n","                 pool_params=None,  # dict, Mandatory if pool_type is not None, otherwise ignored\n","                 act_pool=None,      # str, Mandatory if pool_type is not None, else ignored\n","                 flatten=False,       # Optional: set to True to flatten the outputs\n","                 softmax=False,       # Optional: set to Ture to include softmax layer\n","\n","                 **kwargs):\n","\n","        super(Bundle, self).__init__()\n","\n","        self.serial = serial\n","        self.core_type = core_type\n","        self.core_params = core_params\n","        self.act_core = act_core\n","        self.act_add = act_add\n","        self.pool_type = pool_type\n","        self.pool_params = pool_params\n","        self.act_pool = act_pool\n","        self.flatten = flatten\n","        self.softmax = softmax\n","\n","        self.init_sanity_check()\n","\n","        # Parameters derived from\n","        self.core_layer = None\n","        self.act_core_layer = None\n","        self.act_add_layer = None\n","        self.pool_layer = None\n","        self.act_pool_layer = None\n","        self.flatten_layer = None\n","        self.softmax_layer = None\n","\n","        # Store reference to bundle object here, not just a serial number\n","        self.prev_bundle = None\n","        self.add_bundle = None\n","\n","        if self.core_type == 'conv':\n","            self.core_layer = QConv2DBatchnorm(filters=self.core_params['filters'], kernel_size=self.core_params['kernel_size'], strides=self.core_params['strides'],\n","                                               padding=self.core_params['padding'], kernel_quantizer=self.core_params['kernel_quantizer'], bias_quantizer=self.core_params['bias_quantizer'], use_bias=self.core_params['use_bias'])\n","        else:\n","            self.core_layer = QDense(units=self.core_params['units'], kernel_quantizer=self.core_params['kernel_quantizer'],\n","                               bias_quantizer=self.core_params['bias_quantizer'], use_bias=self.core_params['use_bias'])\n","\n","        self.act_core_layer = QActivation(self.act_core)\n","\n","        if self.act_add is not None:\n","            self.act_add_layer= QActivation(self.act_add)\n","\n","        if self.pool_type == 'max':\n","            self.pool_layer = MaxPooling2D(self.pool_params['size'], strides=self.pool_params['strides'], padding=self.pool_params['padding'])\n","            self.act_pool_layer = QActivation(self.act_pool)\n","        elif self.pool_type == 'avg':\n","            self.pool_layer = QAveragePooling2D(self.pool_params['size'], strides=self.pool_params['strides'], padding=self.pool_params['padding'])\n","            self.act_pool_layer = QActivation(self.act_pool)\n","\n","\n","        if self.flatten:\n","            self.flatten_layer = Flatten()\n","\n","        if self.softmax:\n","            self.softmax_layer = Activation(\"softmax\")\n","\n","        # reference output results, from a given input\n","        self.y_fp = None\n","        self.y_int = self.y_bits = self.y_frac = None\n","        self.post_core = self.post_add = self.post_pool = None # fp only\n","\n","        # For inference, and export only, int, bits, frac\n","        self.x_in = self.a_in = self.x_out = None, None, None  # For the process function!\n","        self.act_core_dict = self.act_add_dict = self.act_pool_dict = None\n","        self.w = self.b = None, None, None\n","        self.bundle_list = None\n","\n","\n","    def init_sanity_check(self):\n","        required_CONV = ['filters', 'kernel_size', 'strides', 'padding', 'kernel_quantizer', 'bias_quantizer', 'use_bias']\n","        required_POOL = ['size', 'strides', 'padding']\n","        required_DENSE = ['units', 'kernel_quantizer', 'bias_quantizer', 'use_bias']\n","\n","        assert (type(self.serial) is int), f\"'{type(config['serial'])} must be an integer!\"\n","\n","        if self.core_type == 'conv':\n","            for i in required_CONV:\n","                assert i in self.core_params, f\"'{i}' must be provided for conv\"\n","        elif self.core_type == 'dense':\n","            for i in required_DENSE:\n","                assert i in self.core_params, f\"'{i}' must be provided for dense\"\n","        else:\n","            raise Exception(self.core_type, \"a core type must be provided, only conv or dense supported for now\")\n","\n","        assert self.act_core is not None, \"Must provide activation for core\"\n","\n","        if self.pool_type is not None:\n","            if self.pool_type not in ['avg', 'max']:\n","                raise Exception(self.pool_type, \"only avg or max supported for now\")\n","            for i in required_POOL:\n","                assert i in self.pool_params, f\"'{i}' must be provided for pooling\"\n","\n","            assert self.act_pool is not None, \"Must provide activation post pooling\"\n","\n","        return\n","\n","    # functions for training\n","    def call(self, x, x_1=None):\n","        if hasattr(x, \"bundleObj\"):\n","            self.prev_bundle = x.bundleObj\n","        else:\n","            self.prev_bundle = -1\n","            # -1 indicates input layer, which do not belong to any bundle\n","\n","        x = self.core_layer(x)\n","        x = self.act_core_layer(x)\n","        if x_1 is not None:\n","            if hasattr(x_1, \"bundleObj\"):\n","                self.add_bundle = x_1.bundleObj\n","            else:\n","                self.add_bundle = -1\n","                # -1 indicates input layer, which do not belong to any bundle\n","            x = Add()([x, x_1])\n","            x = self.act_add_layer(x)\n","        if self.pool_layer:\n","            x = self.pool_layer(x)\n","            x = self.act_pool_layer(x)\n","        if self.flatten_layer:\n","            x = self.flatten_layer(x)\n","        if self.softmax_layer:\n","            x = self.softmax_layer(x)\n","\n","        x.bundleObj = self\n","\n","        return x\n","\n","    # functions to be prepared for exportation\n","    def load_weight_bias(self):\n","        #print('check serial', self.serial)\n","        k = self.core_layer.get_folded_weights()[0] if isinstance(self.core_layer, QConv2DBatchnorm) else self.core_layer.kernel\n","        k = self.core_layer.kernel_quantizer_internal(k).numpy()\n","        k_config = self.core_layer.kernel_quantizer_internal.get_config()\n","\n","        k_frac = k_config['bits']-k_config['integer']-k_config['keep_negative']\n","        k_int = k * 2**k_frac\n","        assert (k_int == k_int.astype(int)).all()\n","        k_int = k_int.astype(int)\n","\n","        self.w = k_int, k_config['bits'], k_frac\n","\n","        if (self.core_type == 'conv' and self.core_params['use_bias']) or (self.core_type == 'dense' and self.core_params['use_bias']):\n","            b = self.core_layer.get_folded_weights()[1] if isinstance(self.core_layer, QConv2DBatchnorm) else self.core_layer.bias\n","            b = self.core_layer.bias_quantizer_internal(b).numpy()\n","            b_config = self.core_layer.bias_quantizer_internal.get_config()\n","            b_frac = b_config['bits']-b_config['integer']-b_config['keep_negative']\n","            b_int = b * 2**b_frac\n","            assert (b_int == b_int.astype(int)).all()\n","            b_int = b_int.astype(int)\n","            self.b = b_int, b_config['bits'], b_frac\n","\n","    def prepare_val(self, y_fp, x_init=None):\n","        self.load_weight_bias()\n","        self.y_fp = y_fp\n","\n","        # input tensor\n","        x_ref = None\n","        if self.serial == 0:\n","            x_ref = x_init\n","        else:\n","            x_ref = self.prev_bundle.y_fp\n","\n","        x = self.core_layer(x_ref)\n","        x = self.act_core_layer(x)\n","        self.post_core = x.numpy()\n","\n","        if self.add_bundle is not None:\n","            x1 = self.add_bundle.y_fp\n","            x = Add()([x, x1])\n","            self.post_add = x.numpy()\n","            x = self.act_add_layer(x)\n","            self.post_add_act = x.numpy()\n","        if self.pool_layer:\n","            x = self.pool_layer(x)\n","            x = self.act_pool_layer(x)\n","            self.post_pool = x.numpy()\n","        if self.flatten_layer:\n","            x = self.flatten_layer(x)\n","        if self.softmax_layer:\n","            x = self.softmax_layer(x)\n","\n","        def extract_act(ilayer):\n","            d = ilayer.quantizer.get_config()\n","            sign_bit = d['keep_negative'] if 'keep_negative' in d else (d['negative_slope'] !=0 if 'negative_slope' in d else (0))\n","            int_bit = d['integer'] if 'integer' in d else 0\n","            frac = d['bits']-int_bit-sign_bit\n","            if isinstance(ilayer.quantizer, quantized_bits):\n","                return {'type':'quant', 'bits':d['bits'], 'frac':frac}\n","            elif 'relu' in str(ilayer.quantizer.__class__):\n","                return {'type':'relu', 'slope':ilayer.quantizer.negative_slope, 'bits':d['bits'], 'frac':frac}\n","            else:\n","                raise Exeption(\"Only relu is suppported!\")\n","\n","        self.act_core_dict = extract_act(self.act_core_layer)\n","        self.y_frac, self.y_bits = self.act_core_dict['frac'], self.act_core_dict['bits']\n","\n","        if self.add_bundle is not None:\n","            self.act_add_dict = extract_act(self.act_add_layer)\n","            self.y_frac, self.y_bits = self.act_add_dict['frac'], self.act_add_dict['bits']\n","\n","        if self.pool_layer:\n","            self.act_pool_dict = extract_act(self.act_pool_layer)\n","            self.y_frac, self.y_bits = self.act_pool_dict['frac'], self.act_pool_dict['bits']\n","\n","        if self.softmax_layer:\n","            self.y_frac, self.y_bits = 0, 1\n","\n","        self.y_int = self.y_fp * 2**self.y_frac\n","\n","        # no point of assertion in case of softmax, exponential used!\n","        if self.softmax is None:\n","            assert (self.y_int == self.y_int.astype(int)).all(), self.serial\n","\n","\n","    def process_val(self, function, x = None):\n","        # x is none: chained mode, read x from prev layer y\n","        # x is not none: independent mode, inputs fed by outputs. Must provide input for the first bundle\n","        if x is not None:\n","            self.x_in = x\n","        else:\n","            # ToDo: do not rely on external(global) variables!\n","            prev_bdl = self.prev_bundle\n","            self.x_in = prev_bdl.y_int, prev_bdl.y_bits, prev_bdl.y_frac\n","            assert self.serial > 0, \"input must be provided manually for the first bundle\"\n","\n","        def quantize(x, bits, frac):\n","            x = x.astype(np.float32)\n","            x /= 2 ** frac\n","            x = np.around(x)\n","            x = np.clip(x, -2**(bits-1), 2**(bits-1)-1)\n","            x = x.astype(int)\n","            return x\n","\n","        x_arr, x_bits, x_frac = self.x_in\n","        x_fp = x_arr / (2**x_frac)\n","\n","        w_arr, w_bits, w_frac = self.w\n","\n","        out_bits, out_frac = x_bits + w_bits, x_frac + w_frac\n","        # ToDo: fix the out bits, addition not considered!\n","        out_arr = function(x_arr, w_arr)\n","\n","        if self.b[0] is not None:\n","            b_arr, b_bits, b_frac = self.b\n","            out_arr += b_arr * 2** (out_frac - b_frac)\n","\n","        if 'strides' in self.core_params and self.core_params['strides'] != (1,1):\n","            SH, SW = self.core_params['strides']\n","            N, XH, XW, C = out_arr.shape\n","            YH, YW = XH//SH, XW//SW\n","            out_arr = out_arr.reshape(N, YH, SH, YW, SW, C)\n","            ind = -1 if w_arr.shape[0] > 1 else 0\n","            out_arr = out_arr[:,:,ind,:,ind,:]\n","\n","        def process_act(in_arr, in_bits, in_frac, act_dict):\n","\n","            if act_dict['type'] == 'quant':\n","                out_arr = quantize(x=in_arr, bits=act_dict['bits'], frac=in_frac-act_dict['frac'])\n","                out_frac = act_dict['frac']\n","                out_bits = act_dict['bits']\n","\n","            elif act_dict['type'] == 'relu':\n","                frac, bits = act_dict['frac'], act_dict['bits']\n","\n","                out_arr = in_arr * 2**(frac-in_frac)\n","                out_arr = np.clip(out_arr, -2**(bits-1), 2**(bits-1)-1)\n","\n","                out_arr = np.maximum(out_arr * act_dict['slope'], out_arr)\n","                out_arr = np.around(out_arr)\n","                out_arr = np.clip(out_arr,-2**(bits-1), 2**(bits-1)-1).astype(int)\n","\n","                out_frac, out_bits = frac, bits\n","            else:\n","                raise Exception('Only relu is supported yet')\n","\n","            return out_arr, out_bits, out_frac\n","\n","        out_arr, out_bits, out_frac = process_act(out_arr, out_bits, out_frac, self.act_core_dict)\n","        assert np.all(out_arr == self.post_core * 2**out_frac)\n","\n","        if self.add_bundle is not None:\n","            part_bdl = self.add_bundle\n","\n","            added_fp = self.post_core+part_bdl.y_fp\n","\n","            a_arr, a_bits, a_frac = part_bdl.y_int, part_bdl.y_bits, part_bdl.y_frac\n","            out_frac_add, out_bits_add = max(out_frac, a_frac), max(out_bits, a_bits)\n","            a_arr_cast = a_arr * 2** (out_frac_add - a_frac)\n","            out_arr_cast = out_arr * 2 **(out_frac_add - out_frac)\n","            out_arr = out_arr_cast.astype(np.int64) + a_arr_cast.astype(np.int64)\n","\n","            out_bits, out_frac = out_bits_add, out_frac_add\n","\n","            assert np.all(out_arr == self.post_add * 2**out_frac)\n","            #print(out_bits, out_frac)\n","            #print(out_arr[0,:,:,0])\n","            if self.act_add:\n","                #out_arr = added_fp * 2**out_frac\n","                out_arr, out_bits, out_frac = process_act(out_arr, out_bits, out_frac, self.act_add_dict)\n","\n","            #print(self.act_add_dict, out_frac)\n","            #print(added_fp[0,:,:,0])\n","            #print(out_arr[0,:,:,0])\n","            #print(self.act_add_dict)\n","\n","            #print(out_frac, self.post_add_act[0,:,:,0]*2**out_frac)\n","            #print(self.post_add[0,:,:,0] * 2**out_frac)\n","            assert np.all(out_arr == self.post_add_act * 2**out_frac)\n","\n","        if self.pool_layer:\n","            if self.pool_type == 'max':\n","                pStride = self.pool_params['strides']\n","                pSize = self.pool_params['size']\n","\n","                def findMax(InArray, p, q):\n","                    results = np.zeros((InArray.shape[0], InArray.shape[3]))\n","                    results -= math.inf\n","                    for i in range(p, p+pSize[0]):\n","                        for j in range(q, q+pSize[1]):\n","                            if i >=0 and j>=0 and i < InArray.shape[1] and j < InArray.shape[2]:\n","                                cand = InArray[:,i,j,:]\n","                                results = np.maximum(results, cand)\n","                    return results\n","\n","                def HotFixMaxPool2D(InArray):\n","                    if pStride[0]!=pStride[1] or pSize[0]!=pSize[1]:\n","                        raise Exception('Only square stride and size is supported')\n","                    if pSize[0]/2 == 0:\n","                        raise Exception('Maxpool size should be odd')\n","\n","                    pad = (pSize[0]-1)//2\n","\n","                    inShape = InArray.shape\n","                    assert len(inShape) == 4\n","                    OutArray = np.zeros((inShape[0], inShape[1]//pStride[0], inShape[2]//pStride[1], inShape[3]))\n","                    # Start point, should include pad\n","                    st_p, st_q = -pad, -pad\n","\n","                    for i in range(OutArray.shape[1]):\n","                        for j in range(OutArray.shape[2]):\n","                            p, q = st_p + i*pStride[0] + pStride[0]-1, st_q + j*pStride[1] + pStride[1]-1\n","                            OutArray[:,i,j,:] = findMax(InArray, p, q)\n","\n","                    return OutArray\n","\n","                out_arr = HotFixMaxPool2D(out_arr).astype(int)\n","\n","            elif self.pool_type == 'avg':\n","                assert self.pool_params['size'] == self.pool_params['strides']\n","                KH, KW = self.pool_params['size']\n","                N, H, W, C = out_arr.shape\n","                out_arr = out_arr.reshape(N, H//KH, KH, W//KW, KW, C).mean(axis=(2,4))\n","                # NO need for clipping, as act_pool in place!\n","\n","            if self.act_pool:\n","                out_arr, out_bits, out_frac = process_act(out_arr, out_bits, out_frac, self.act_pool_dict)\n","            assert np.all(out_arr == self.post_pool * 2**out_frac)\n","\n","        if self.flatten:\n","            out_arr = out_arr.reshape(out_arr.shape[0],-1)\n","\n","\n","\n","        if self.softmax:\n","            out_arr = out_arr / 2**out_frac\n","            exp = np.exp(out_arr - out_arr.max())\n","            out_arr = exp/np.sum(exp, axis=1)[0]\n","            assert np.all(np.argmax(self.y_int, axis=-1) == np.argmax(out_arr, axis=-1))\n","        else:\n","            assert np.all(out_arr == self.y_int)\n","\n","\n","        return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"XMU8oLf7a_OC","outputId":"afc2212b-e305-4219-be8c-e37f82e4a8e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input (InputLayer)             [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," q_activation (QActivation)     (None, 32, 32, 3)    0           ['input[0][0]']                  \n","                                                                                                  \n"," bundle (Bundle)                (None, 16, 16, 64)   9729        ['q_activation[0][0]']           \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm (QConv2DBat  multiple          9729        []                               |\n","| chnorm)                                                                                        |\n","|                                                                                                |\n","| q_activation_1 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| max_pooling2d (MaxPooling2D)  multiple            0           []                               |\n","|                                                                                                |\n","| q_activation_2 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_1 (Bundle)              (None, 16, 16, 64)   37185       ['bundle[0][0]']                 \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_1 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_3 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_2 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_1[0][0]',               \n","                                                                  'bundle[0][0]']                 \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_2 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_4 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_activation_5 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_3 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_2[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_3 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_6 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_4 (Bundle)              (None, 16, 16, 64)   37185       ['bundle_3[0][0]',               \n","                                                                  'bundle_2[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_4 (QConv2DB  multiple          37185       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_7 (QActivation)  multiple            0           []                               |\n","|                                                                                                |\n","| q_activation_8 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_5 (Bundle)              (None, 8, 8, 128)    74369       ['bundle_4[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_5 (QConv2DB  multiple          74369       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_9 (QActivation)  multiple            0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_6 (Bundle)              (None, 8, 8, 128)    148097      ['bundle_5[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_6 (QConv2DB  multiple          148097      []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_10 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_7 (Bundle)              (None, 8, 8, 128)    8833        ['bundle_4[0][0]',               \n","                                                                  'bundle_6[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_7 (QConv2DB  multiple          8833        []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_11 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_12 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_8 (Bundle)              (None, 8, 8, 128)    148097      ['bundle_7[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_8 (QConv2DB  multiple          148097      []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_13 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_9 (Bundle)              (None, 8, 8, 128)    17025       ['bundle_8[0][0]',               \n","                                                                  'bundle_7[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_9 (QConv2DB  multiple          17025       []                               |\n","| atchnorm)                                                                                      |\n","|                                                                                                |\n","| q_activation_14 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_15 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_10 (Bundle)             (None, 4, 4, 256)    296193      ['bundle_9[0][0]']               \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_10 (QConv2D  multiple          296193      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_16 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_11 (Bundle)             (None, 4, 4, 256)    591105      ['bundle_10[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_11 (QConv2D  multiple          591105      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_17 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_12 (Bundle)             (None, 4, 4, 256)    34049       ['bundle_9[0][0]',               \n","                                                                  'bundle_11[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_12 (QConv2D  multiple          34049       []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_18 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_19 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_13 (Bundle)             (None, 4, 4, 256)    591105      ['bundle_12[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_13 (QConv2D  multiple          591105      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_20 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_14 (Bundle)             (None, 4, 4, 256)    66817       ['bundle_13[0][0]',              \n","                                                                  'bundle_12[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_14 (QConv2D  multiple          66817       []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_21 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_22 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_15 (Bundle)             (None, 2, 2, 512)    1182209     ['bundle_14[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_15 (QConv2D  multiple          1182209     []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_23 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_16 (Bundle)             (None, 2, 2, 512)    2361857     ['bundle_15[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_16 (QConv2D  multiple          2361857     []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_24 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_17 (Bundle)             (None, 2, 2, 512)    133633      ['bundle_14[0][0]',              \n","                                                                  'bundle_16[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_17 (QConv2D  multiple          133633      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_25 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_26 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_18 (Bundle)             (None, 2, 2, 512)    2361857     ['bundle_17[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_18 (QConv2D  multiple          2361857     []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_27 (QActivation)  multiple           0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_19 (Bundle)             (None, 512)          264705      ['bundle_18[0][0]',              \n","                                                                  'bundle_17[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_conv2d_batchnorm_19 (QConv2D  multiple          264705      []                               |\n","| Batchnorm)                                                                                     |\n","|                                                                                                |\n","| q_activation_28 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_activation_29 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| q_average_pooling2d (QAverageP  multiple          0           []                               |\n","| ooling2D)                                                                                      |\n","|                                                                                                |\n","| q_activation_30 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| flatten (Flatten)            multiple             0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"," bundle_20 (Bundle)             (None, 10)           5130        ['bundle_19[0][0]']              \n","|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n","| q_dense (QDense)             multiple             5130        []                               |\n","|                                                                                                |\n","| q_activation_31 (QActivation)  multiple           0           []                               |\n","|                                                                                                |\n","| activation (Activation)      multiple             0           []                               |\n","¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n","==================================================================================================\n","Total params: 8,443,550\n","Trainable params: 8,433,930\n","Non-trainable params: 9,620\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["x = x_in = Input(input_shape, name='input')\n","x = QActivation(q_0)(x)\n","\n","x = x1 = Bundle(0, 'conv', {'filters':64, 'kernel_size':(7,7), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_0,\n","          None, 'max', {'size':(3,3), 'strides':(1,1), 'padding':'same'}, q_0)(x)\n","# block 0\n","x = Bundle(1, 'conv', {'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_0)(x)\n","x = x1 = Bundle(2, 'conv', {'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_1, a_0)(x, x1)\n","\n","x = Bundle(3, 'conv', {'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_0)(x)\n","x = x1 = Bundle(4, 'conv', {'filters':64, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_1, a_1)(x, x1)\n","# block 1\n","x1 = Bundle(5, 'conv', {'filters':128, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_1)(x1)\n","x1 = Bundle(6, 'conv', {'filters':128, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, q_2)(x1)\n","x = x1 = Bundle(7, 'conv', {'filters':128, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_2, a_2)(x, x1)\n","\n","x = Bundle(8, 'conv', {'filters':128, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_1)(x)\n","x = x1 = Bundle(9, 'conv', {'filters':128, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_2, a_2)(x, x1)\n","\n","#block 2\n","x1 = Bundle(10, 'conv', {'filters':256, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_1)(x1)\n","x1 = Bundle(11, 'conv', {'filters':256, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, q_2)(x1)\n","x = x1 = Bundle(12, 'conv', {'filters':256, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_3, a_3)(x, x1)\n","\n","x = Bundle(13, 'conv', {'filters':256, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_2)(x)\n","x = x1 = Bundle(14, 'conv', {'filters':256, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_3, a_3)(x, x1)\n","\n","#block 3\n","x1 = Bundle(15, 'conv', {'filters':512, 'kernel_size':(3,3), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_2)(x1)\n","x1 = Bundle(16, 'conv', {'filters':512, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, q_3)(x1)\n","x = x1 = Bundle(17, 'conv', {'filters':512, 'kernel_size':(1,1), 'strides':(2,2), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_3, a_3)(x, x1)\n","\n","x = Bundle(18, 'conv', {'filters':512, 'kernel_size':(3,3), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS}, a_2)(x)\n","x = Bundle(19, 'conv', {'filters':512, 'kernel_size':(1,1), 'strides':(1,1), 'padding':'same', 'kernel_quantizer':q_0, 'bias_quantizer':q_0, 'use_bias':USE_BIAS},\n","                 q_3, a_3, 'avg', {'size':(2,2), 'strides':(2,2), 'padding':'valid'}, q_3, True)(x, x1)\n","\n","x = Bundle(20, 'dense', {'units':10, 'kernel_quantizer':q_2, 'bias_quantizer':q_2, 'use_bias':USE_BIAS}, q_3, softmax=True)(x)\n","\n","model = Model(inputs=x_in, outputs=x)\n","print(model.summary(expand_nested=True))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qSxuQVKda_OC","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning rate:  0.001\n"]}],"source":["def lr_schedule(epoch):\n","    \"\"\"\n","  Bundles_pre_trainearning Rate Schedule\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","    # Arguments\n","        epoch (int): The number of epochs\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    # initial_lr = 1e-4\n","    # lr_decay = 0.99\n","    # lr = initial_lr * (lr_decay ** epoch)\n","    lr = 1e-3 # default 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 150:\n","        lr *= 1e-2\n","    elif epoch > 100:\n","        lr *= 1e-1\n","    elif epoch > 50:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","preamble = ''\n","model_file_path = preamble+'resnet18.h5'\n","checkpoint = ModelCheckpoint(filepath=model_file_path,\n","                                     monitor='val_acc',\n","                                     verbose=1,\n","                                     save_best_only=True)\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                                       cooldown=0,\n","                                       patience=5,\n","                                       min_lr=0.5e-6)\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","callbacks = [checkpoint, lr_reducer, lr_scheduler]\n","\n","NB_EPOCH = 200\n","BATCH_SIZE = 256\n","VERBOSE = 1\n","VALIDATION_SPLIT = 0.1\n","RELU_NEG_SLOPE = 0.125\n","\n","model.compile(loss='categorical_crossentropy',\n","            optimizer=Adam(learning_rate=lr_schedule(0)), metrics=['acc'])\n","\n","# model.fit(x_train, y_train,\n","#                     batch_size=BATCH_SIZE,\n","#                     epochs=NB_EPOCH,\n","#                     validation_data=(x_test, y_test),\n","#                     shuffle=True,\n","#                     callbacks=callbacks)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xwApBeKxa_OD","tags":[]},"outputs":[],"source":["#arch = model.to_json()\n","#with open(\"bundle_resnet_18.json\", 'w') as arch_file:\n","#    arch_file.write(arch)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"I6_vo0gYa_OD"},"outputs":[],"source":["def conv(x,w):\n","    x = x.astype(np.int32)\n","    w = w.astype(np.int32)\n","    return tf.keras.backend.conv2d(x, w, padding='same').numpy()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VPp5nSPYa_OD","outputId":"968bf09a-7290-4544-fcda-e458b29b7777"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4, 32, 32, 3)\n","bundle\n"]},{"name":"stdout","output_type":"stream","text":["bundle_1\n","bundle_2\n","bundle_3\n","bundle_4\n","bundle_5\n","bundle_6\n","bundle_7\n","bundle_8\n","bundle_9\n","bundle_10\n","bundle_11\n","bundle_12\n","bundle_13\n","bundle_14\n","bundle_15\n","bundle_16\n","bundle_17\n","bundle_18\n","bundle_19\n","bundle_20\n"]}],"source":["XN = 4\n","x = np.random.randn(XN, *model.input.shape[1:])\n","x = np.clip(x, -1.0, 1.0)\n","print(x.shape)\n","\n","pre_layer = model.layers[1]\n","temp_model = Model(inputs=model.input, outputs=pre_layer.output)\n","x_init = temp_model(x, training=False)\n","\n","for i, layer in enumerate(model.layers[2:]): # disregard input and initial quant layers\n","    print(layer.name)\n","    temp_model = Model(inputs=model.input, outputs=layer.output)\n","    y = temp_model(x, training=False).numpy()\n","    x_init = x_init if i == 0 else None\n","    layer.prepare_val(y, x_init)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"LGQRt23Na_OD"},"outputs":[],"source":["init_layer = model.layers[1]\n","temp_model = Model(inputs=model.input, outputs=init_layer.output)\n","x_init = temp_model(x, training=False).numpy()\n","x_init_bits = 8\n","x_init_frac = 7\n","x_init_arr = x_init * 2**x_init_frac\n","\n","model.layers[2].process_val(conv, (x_init_arr, x_init_bits, x_init_frac))\n","for j in range(3, len(model.layers)-1):\n","    model.layers[j].process_val(conv)\n","\n","model.layers[-1].process_val((lambda x, w : x @ w))"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UcO2nguRa_OD"},"outputs":[{"data":{"text/plain":["array([[[[122.,  72.,  30., ..., 127., 127., 100.],\n","         [122.,  72.,  84., ..., 127., 127., 100.],\n","         [127., 127.,  84., ..., 100., 127.,  91.],\n","         ...,\n","         [116.,  77., 127., ..., 127., 127., 110.],\n","         [116., 127., 127., ..., 127., 119., 127.],\n","         [116., 127., 127., ..., 127., 119., 127.]],\n","\n","        [[127.,  72.,  30., ..., 127., 127., 127.],\n","         [127.,  72.,  84., ..., 127., 127., 127.],\n","         [127., 127.,  84., ..., 127., 127., 127.],\n","         ...,\n","         [116.,  77., 127., ..., 127., 127., 110.],\n","         [116., 127., 127., ..., 127., 119., 127.],\n","         [116., 127., 127., ..., 127., 119., 127.]],\n","\n","        [[127.,  -1., 127., ..., 127., 127., 127.],\n","         [127.,   0., 127., ..., 127., 127., 127.],\n","         [ 72.,  55., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 126., 127., ..., 127.,  85., 110.],\n","         [127., 126., 127., ..., 127., 119., 127.],\n","         [127.,  91., 127., ..., 127., 119., 127.]],\n","\n","        ...,\n","\n","        [[127., 125.,  64., ..., 127., 127., 127.],\n","         [127., 125., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 126., 127., 127.],\n","         [127., 127., 127., ...,  36., 127., 127.]],\n","\n","        [[127., 125.,  11., ..., 127., 124., 127.],\n","         [127., 125., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127.,  53., ..., 103., 123., 127.],\n","         [127., 127.,  53., ..., 103.,  88., 127.],\n","         [106., 127.,  41., ...,  63.,  88., 117.]],\n","\n","        [[ 80., 125.,  11., ..., 127., 124., 127.],\n","         [127., 125., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127.,  53., ..., 103., 123., 127.],\n","         [106., 127.,  53., ..., 103.,  88., 127.],\n","         [106., 127.,  41., ...,  63.,  88., 117.]]],\n","\n","\n","       [[[ 65., 127., 114., ...,  91., 127.,  67.],\n","         [127., 127., 114., ...,  91., 127., 127.],\n","         [127., 127., 114., ...,  91., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 127.,  94.,  57.],\n","         [127., 127., 127., ..., 127.,  72., 127.],\n","         [127.,  84.,  -3., ..., 127.,  72., 127.]],\n","\n","        [[125., 127., 114., ...,  91., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 127.,  94., 127.],\n","         [127., 127., 127., ..., 127.,  72., 127.],\n","         [127., 127., 127., ..., 127.,  72., 127.]],\n","\n","        [[125., 127., 127., ...,  91., 127., 127.],\n","         [125., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127.,  84., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127.,  71., 127.]],\n","\n","        ...,\n","\n","        [[ 65.,  32., 127., ..., 127.,  94., 127.],\n","         [ 65., 127., 127., ..., 127., 127., 127.],\n","         [ 65., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 126.,  66., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.]],\n","\n","        [[127.,  43., 127., ..., 127.,  94., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [ 43., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 126.,  66., 127.],\n","         [127., 127., 127., ..., 126.,  76., 127.],\n","         [127., 127., 127., ..., 126.,  76., 127.]],\n","\n","        [[127.,  43., 127., ..., 127.,  87.,  57.],\n","         [127., 127., 127., ..., 127.,  87.,  57.],\n","         [ 36., 127., 127., ..., 127.,  24.,  55.],\n","         ...,\n","         [127., 127., 127., ...,  57.,  20., 127.],\n","         [127., 127., 127., ...,  38.,  20., 127.],\n","         [127., 127., 127., ...,   6.,  20., 127.]]],\n","\n","\n","       [[[ -5., 127.,  55., ...,   0.,  24., 123.],\n","         [102., 127.,  55., ...,  33., 127., 123.],\n","         [127.,  88., 127., ...,  33., 127., 123.],\n","         ...,\n","         [127., 127., 115., ..., 127., 127., 127.],\n","         [127., 127., 115., ..., 127., 127., 127.],\n","         [127., 127.,  59., ...,  36., 102., 127.]],\n","\n","        [[ -5., 127., 127., ...,  97.,  24., 123.],\n","         [102., 127., 127., ...,  97., 127., 123.],\n","         [127., 127., 127., ..., 127., 127., 123.],\n","         ...,\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127.,  59., ..., 127., 127., 127.]],\n","\n","        [[124., 127., 127., ...,  97.,  45., 127.],\n","         [124., 127., 127., ...,  97.,  52., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [ 89.,  70.,  86., ..., 127., 127., 127.]],\n","\n","        ...,\n","\n","        [[ 74., 127., 119., ..., 127.,  46.,  53.],\n","         [ 90., 127., 127., ..., 127.,  46.,  53.],\n","         [ 90.,  68., 127., ..., 127., 127.,  26.],\n","         ...,\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 121., ..., 127., 127., 127.],\n","         [127., 127., 121., ..., 127., 127., 127.]],\n","\n","        [[ 74.,  68., 119., ..., 127.,  11.,  86.],\n","         [ 90.,  68., 119., ..., 127.,  11.,  86.],\n","         [ 90.,  68., 119., ..., 127.,  17.,  86.],\n","         ...,\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 103., ..., 127., 127., 127.],\n","         [127.,  93.,  -1., ..., 127., 127.,  -1.]],\n","\n","        [[ 48.,  68., 119., ..., 127.,  11.,  86.],\n","         [ 90.,  68., 119., ..., 127.,  11.,  86.],\n","         [ 90.,  68., 119., ..., 127.,  17.,  86.],\n","         ...,\n","         [127., 127., 127., ...,  44., 127.,  -4.],\n","         [127., 127.,  -1., ..., 127., 127.,  -1.],\n","         [ -2.,  93.,  -1., ..., 127.,  58.,  -1.]]],\n","\n","\n","       [[[127., 127.,  72., ...,  49.,  68., 127.],\n","         [127., 127.,  72., ...,  49.,  68., 127.],\n","         [127., 127., 119., ...,  77., 105., 127.],\n","         ...,\n","         [127.,  67., 127., ...,  97.,  80., 127.],\n","         [127.,  67., 127., ...,  30.,  80., 127.],\n","         [127.,  46., 127., ...,  30.,  80., 127.]],\n","\n","        [[127., 127.,  91., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127.,  67., 127., ...,  97.,  80., 127.],\n","         [127.,  67., 127., ...,  30.,  80., 127.],\n","         [127.,  57., 127., ...,  30.,  80., 127.]],\n","\n","        [[127., 127.,  91., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127.,  99., 107., ...,  97.,  80., 127.],\n","         [127.,  99., 127., ...,  64.,  80., 127.],\n","         [127.,  99., 127., ...,  64.,  80., 127.]],\n","\n","        ...,\n","\n","        [[127., 127., 127., ..., 127.,  94., 127.],\n","         [127., 127., 127., ..., 127.,  94., 127.],\n","         [127., 127., 127., ..., 127., 127., 127.],\n","         ...,\n","         [127., 127., 127., ..., 114., 127.,  99.],\n","         [127., 127., 127., ..., 127., 127.,  30.],\n","         [127., 127., 127., ..., 127., 127.,  30.]],\n","\n","        [[127., 127., 127., ..., 127., 118., 127.],\n","         [127., 127., 127., ..., 127., 118., 127.],\n","         [127., 127., 127., ..., 127., 118., 127.],\n","         ...,\n","         [127., 127., 127., ..., 114., 127.,  99.],\n","         [127., 127., 127., ..., 127., 127.,  30.],\n","         [127., 127.,  69., ..., 127., 127.,  30.]],\n","\n","        [[103.,  16., 127., ..., 127., 118., 127.],\n","         [103., 127., 127., ..., 127., 118., 127.],\n","         [103., 127., 127., ...,  84., 118., 127.],\n","         ...,\n","         [127., 127., 127., ..., 114., 127.,  99.],\n","         [127., 127., 127., ..., 127., 127.,  30.],\n","         [ 62., 127.,  69., ..., 127., 127.,  30.]]]], dtype=float32)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[2].y_int"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'Bundle' object has no attribute 'o_arr'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mo_arr\n","\u001b[1;31mAttributeError\u001b[0m: 'Bundle' object has no attribute 'o_arr'"]}],"source":["model.layers[2].o_arr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
