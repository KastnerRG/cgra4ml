{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Softmax, Add, MaxPooling2D\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "\n",
    "from pynq import Overlay\n",
    "from pynq import allocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "class Bundle:\n",
    "    def __init__(self, type, strides, add_bundle_i, flatten, softmax, bundles, last_layer_name, prev_layer_name, x, w, b, y, quant_details, act_details, pool_details, o_arr, o_bits, o_frac):\n",
    "\n",
    "        self.type = type        \n",
    "        self.last_layer_name = last_layer_name\n",
    "        self.softmax = softmax\n",
    "        self.strides = strides\n",
    "\n",
    "        '''\n",
    "        Find prev bundle\n",
    "        '''\n",
    "        self.prev_bundle_i, self.prev_bundle = None, None\n",
    "        for i, bundle in enumerate(bundles):\n",
    "            if bundle.last_layer_name == prev_layer_name:\n",
    "                self.prev_bundle_i, self.prev_bundle  = i, bundle\n",
    "\n",
    "        self.add_bundle  = bundles[add_bundle_i] if add_bundle_i else None\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.y = y\n",
    "        self.f = flatten\n",
    "        # self.quant = quant\n",
    "        self.quant_details = quant_details\n",
    "        self.act_details = act_details\n",
    "        self.pool_details = pool_details\n",
    "\n",
    "        '''\n",
    "        Bundle output\n",
    "        '''\n",
    "        if softmax:\n",
    "            self.o_arr, self.o_bits, self.o_frac = o_arr, 1, 0\n",
    "        else:\n",
    "            self.o_arr, self.o_bits, self.o_frac = o_arr, o_bits, o_frac\n",
    "\n",
    "\n",
    "        if self.type == 'conv':\n",
    "            self.KH, self.KW, self.CI, self.CO = self.w[0].shape\n",
    "            self.XN, self.XH, self.XW, self.CI = self.x[0].shape\n",
    "            self.XN, self.YH, self.YW, _       = self.y[0].shape\n",
    "            self.SH                            = self.XH//self.YH\n",
    "            self.SW                            = self.XW//self.YW\n",
    "            self.RAM_WEIGHTS                   = self.KH*self.CI\n",
    "            self.RAM_EDGES                     = self.CI* self.XW* int(np.ceil(self.XH//self.XN-1)) if self.KH != 0 else 0\n",
    "        else:\n",
    "            self.CI, self.CO = self.w[0].shape\n",
    "            self.XH, self.CI = self.x[0].shape\n",
    "            self.SH = self.SW = self.XN = self.KH = self.KW = self.XW = self.YW = 1\n",
    "            self.YH = self.XH\n",
    "            self.RAM_WEIGHTS = 0 #self.KH*self.CI # need to update\n",
    "            self.RAM_EDGES = 0\n",
    "\n",
    "    def process(self, function, x_arr):\n",
    "        x_bits, x_frac = self.x[1:]\n",
    "        w_arr, w_bits, w_frac = self.w\n",
    "\n",
    "        out_arr = function(x_arr, self.w[0])\n",
    "        return self.post_process(out_arr)\n",
    "\n",
    "\n",
    "    def post_process(self, out_arr):\n",
    "\n",
    "        def quantize(x, bits, frac):\n",
    "            x = x.astype(np.float32)\n",
    "            x /= 2 ** frac\n",
    "            x = np.around(x)\n",
    "            x = np.clip(x, -2**(bits-1), 2**(bits-1)-1)\n",
    "            x = x.astype(int)\n",
    "            return x\n",
    "\n",
    "        x_bits, x_frac = self.x[1:]\n",
    "        w_bits, w_frac = self.w[1:]\n",
    "        out_bits, out_frac = x_bits + w_bits, x_frac + w_frac\n",
    "\n",
    "        if self.b[0] is not None:\n",
    "            b_arr, b_bits, b_frac = self.b\n",
    "            out_arr += b_arr * 2** (out_frac - b_frac)\n",
    "\n",
    "        if self.strides:\n",
    "            SH, SW = self.strides\n",
    "            N, XH, XW, C = out_arr.shape\n",
    "            YH, YW = XH//SH, XW//SW\n",
    "            out_arr = out_arr.reshape(N, YH, SH, YW, SW, C)\n",
    "            out_arr = out_arr[:,:,-1,:,-1,:]\n",
    "\n",
    "        if self.quant_details:\n",
    "            out_arr = quantize(x=out_arr, bits=self.quant_details['bits'], frac=out_frac-self.quant_details['frac'])\n",
    "            out_frac = out_frac-self.quant_details['frac']\n",
    "            out_bits = self.quant_details['bits']\n",
    "\n",
    "        if self.add_bundle:\n",
    "            a_arr, a_bits, a_frac = self.add_bundle.out, self.add_bundle.o_bits, self.add_bundle.o_frac\n",
    "            out_arr += a_arr * 2** (out_frac - a_frac)\n",
    "\n",
    "        if self.act_details:\n",
    "            frac, bits = self.act_details['frac'], self.act_details['bits']\n",
    "\n",
    "            if self.act_details['type'] == 'relu':\n",
    "                out_arr = out_arr/2**(out_frac-frac)\n",
    "                out_arr = np.clip(out_arr,-2**(bits-1), 2**(bits-1)-1)\n",
    "\n",
    "                out_arr = np.maximum(out_arr * self.act_details['slope'], out_arr)\n",
    "                out_arr = np.around(out_arr)\n",
    "                out_arr = np.clip(out_arr,-2**(bits-1), 2**(bits-1)-1).astype(int)\n",
    "\n",
    "                out_frac, out_bits = frac, bits\n",
    "\n",
    "            else:\n",
    "                raise Exception('Only relu is supported yet')\n",
    "\n",
    "        if self.pool_details:\n",
    "            if self.pool_details['type'] == 'max':\n",
    "                import math\n",
    "                Stride = 2\n",
    "\n",
    "                def findMax(InArray, p, q):\n",
    "                    results = np.zeros((InArray.shape[0], InArray.shape[3]))\n",
    "                    results -= math.inf\n",
    "                    for i in range(p, p+3):\n",
    "                        for j in range(q, q+3):\n",
    "                            if i >=0 and j>=0 and i < InArray.shape[1] and j < InArray.shape[2]:\n",
    "                                cand = InArray[:,i,j,:]\n",
    "                                results = np.maximum(results, cand)\n",
    "                    return results\n",
    "                def HotFixMaxPool2D(InArray):\n",
    "                    pad = 1\n",
    "                    inShape = InArray.shape\n",
    "                    assert len(inShape) == 4\n",
    "                    OutArray = np.zeros((inShape[0], (inShape[1]+pad)//Stride, (inShape[2]+pad)//Stride, inShape[3]))\n",
    "                    for i in range(OutArray.shape[1]):\n",
    "                        for j in range(OutArray.shape[2]):\n",
    "                            # p, q = i*Stride-1, j*Stride-1\n",
    "                            p, q = i*Stride, j*Stride\n",
    "                            OutArray[:,i,j,:] = findMax(InArray, p, q)\n",
    "                    return OutArray\n",
    "                \n",
    "                out_arr = HotFixMaxPool2D(out_arr).astype(int)\n",
    "\n",
    "            elif self.pool_details['type'] == 'avg':\n",
    "                assert self.pool_details['size'] == self.pool_details['strides']\n",
    "                KH, KW = self.pool_details['size']\n",
    "                N, H, W, C = out_arr.shape\n",
    "                out_arr = out_arr.reshape(N, H//KH, KH, W//KW, KW, C).mean(axis=(2,4))\n",
    "\n",
    "                bits = self.pool_details['bits']\n",
    "                out_arr = np.clip(out_arr,-2**(bits-1), 2**(bits-1)-1)\n",
    "                out_arr = np.around(out_arr)\n",
    "                out_arr = np.clip(out_arr,-2**(bits-1), 2**(bits-1)-1).astype(int)\n",
    "            \n",
    "        if self.flatten:\n",
    "            out_arr = out_arr.reshape(out_arr.shape[0],-1)\n",
    "\n",
    "        if self.softmax:\n",
    "            out_arr = out_arr / 2**out_frac\n",
    "            exp = np.exp(out_arr - out_arr.max())\n",
    "            out_arr = exp/np.sum(exp, axis=1)[0]\n",
    "        \n",
    "        self.out = out_arr\n",
    "        return out_arr\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def get_compile_params(bundles, ROWS, COLS):\n",
    "\n",
    "        def clog2(x):\n",
    "            return int(np.ceil(np.log2(x)))\n",
    "        \n",
    "        IN_BITS               = 64\n",
    "        CONFIG_BEATS          = 1\n",
    "        X_BITS = K_BITS       = max([b.x[1] for b in bundles])\n",
    "        KW_MAX                = max([b.KW   for b in bundles])\n",
    "        KH_MAX                = max([b.KH   for b in bundles])\n",
    "        SW_MAX                = max([b.SW   for b in bundles])\n",
    "        SH_MAX                = max([b.SH   for b in bundles])\n",
    "        CI_MAX                = max([b.CI   for b in bundles])\n",
    "        XW_MAX                = max([b.XW   for b in bundles])\n",
    "        XH_MAX                = max([b.XH   for b in bundles])\n",
    "        XN_MAX                = max([b.XN   for b in bundles])\n",
    "        BRAM_WEIGHTS_DEPTH    = max([b.RAM_WEIGHTS + CONFIG_BEATS for b in bundles])\n",
    "        RAM_EDGES_DEPTH       = max([b.RAM_EDGES                  for b in bundles])\n",
    "        \n",
    "        L_MAX                 = clog2(XH_MAX//ROWS)\n",
    "        X_PAD                 = clog2(KH_MAX//2)\n",
    "        BITS_KW2              = clog2((KW_MAX+1)/2)\n",
    "        BITS_KH2              = clog2((KH_MAX+1)/2)\n",
    "        BITS_SW               = clog2(SW_MAX)\n",
    "        BITS_SH               = clog2(SH_MAX)\n",
    "        BITS_CIN_MAX          = clog2(CI_MAX)\n",
    "        BITS_COLS_MAX         = clog2(XW_MAX)\n",
    "        BITS_BLOCKS_MAX       = clog2( L_MAX)\n",
    "        BITS_XN_MAX           = clog2(XN_MAX)\n",
    "        BITS_BRAM_WEIGHTS_ADDR= clog2(BRAM_WEIGHTS_DEPTH)\n",
    "\n",
    "        params = locals()\n",
    "        params = {k:params[k] for k in params if not ('__' in k or k in ['bundles', 'params', 'clog2'])}\n",
    "        c = namedtuple('Compile', params)(**params)\n",
    "        return c\n",
    "\n",
    "    def export (self):\n",
    "\n",
    "        if self.type != 'conv':\n",
    "            print('Conv -> Dense Reshape')\n",
    "            CI, CO = self.w[0].shape\n",
    "            XN, _ = self.x[0].shape\n",
    "            self.w[0] = self.w[0].reshape(1,1,CI,CO) # (CI,CO) -> (KH,KW,CI,CO)\n",
    "            self.x[0] = self.x[0].reshape(XN,1,1,CI) # (XN,CI) -> (XN, XH, XW, CI)\n",
    "            self.y[0] = self.y[0].reshape(XN,1,1,CO) # (XN,CI) -> (XN, XH, XW, CI)\n",
    "        \n",
    "        self.c = c\n",
    "        self.r = self.get_runtime_params(self.c, self.w[0], self.x[0], self.y[0])\n",
    "        self.r = self.create_headers(self.c, self.r)\n",
    "\n",
    "        print(self.r)\n",
    "        self.check_sparsity(self.w[0], self.x[0])\n",
    "\n",
    "        self.we = self.reorder_w_q2e_conv(self.w[0], self.c, self.r)\n",
    "        self.ye_exp_shape = (self.r.IT, self.r.XN, self.r.L, self.r.XW*self.r.CO_PRL, c.ROWS)\n",
    "        self.ye_hw = np.zeros(self.ye_exp_shape)\n",
    "        self.num_t = self.we.shape[0] # iterations\n",
    "\n",
    "        self.r = self.r._asdict()\n",
    "        self.c = self.c._asdict()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_runtime_params(c, w, x, y):\n",
    "\n",
    "        SW = SH = 1 # for bundle\n",
    "        KH, KW, CI, CO = w.shape\n",
    "        print('weights initial (KH, KW, CI, CO) =', w.shape)\n",
    "\n",
    "        CO_PRL         = c.COLS * SW // KW                        # SW cols are processed in parallel\n",
    "        EG             = int(np.floor( c.COLS / (KW + SW - 1)))   # elastic groups\n",
    "        IT             = int(np.ceil( CO / (SW*EG)))              # iterations needed\n",
    "        CO_PAD         = IT * CO_PRL                              # output cols padded\n",
    "\n",
    "        print(f'{KH=}, {KW=}, {CI=}, {CO=}, {CO_PRL=}, {EG=}, {IT=}, {CO_PAD}')\n",
    "\n",
    "        XN, XH, XW, CI = x.shape\n",
    "        print('initial (XN, XH, XW, CI)=', x.shape)\n",
    "        SH_OUT, SW_OUT = x.shape[1]//y.shape[1], x.shape[2]//y.shape[2]\n",
    "\n",
    "        LH     = c.ROWS*SH              # Block height\n",
    "        L      = int(np.ceil(XH/LH))    # Blocks\n",
    "        XH_PAD = LH*L\n",
    "        BRAM_WEIGHTS_ADDR_MAX  = c.CONFIG_BEATS + SW*KH*CI-1\n",
    "\n",
    "        '''\n",
    "        Pack all local variables into a namedtuple\n",
    "        '''\n",
    "        params = locals()\n",
    "        params = {k:params[k] for k in params if not ('__' in k or k in ['w', 'x', 'y', 'c', 'params'])}\n",
    "        print (params)\n",
    "        r = namedtuple('Runtime', params)(**params)\n",
    "        return r\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def create_headers(c, r):\n",
    "        '''\n",
    "        Create headers\n",
    "        '''\n",
    "        def pack_bits(arr):\n",
    "            sum_width = 0\n",
    "            packed = 0\n",
    "            for val, width in arr:\n",
    "                packed |= val << sum_width\n",
    "                sum_width += width\n",
    "            return packed\n",
    "        \n",
    "        ''' Weights Config'''\n",
    "        w_config = pack_bits([\n",
    "            (r.KW//2, c.BITS_KW2),\n",
    "            (r.CI-1 , c.BITS_CIN_MAX),\n",
    "            (r.XW-1 , c.BITS_COLS_MAX),\n",
    "            (r.L -1 , c.BITS_BLOCKS_MAX),\n",
    "            (r.XN-1 , c.BITS_XN_MAX),\n",
    "            (r.BRAM_WEIGHTS_ADDR_MAX, c.BITS_BRAM_WEIGHTS_ADDR)\n",
    "        ])\n",
    "        w_config = format(w_config, f'#0{c.IN_BITS}b')\n",
    "        w_config_words = [int(w_config[i:i+c.K_BITS], 2) for i in range(0, len(w_config), c.K_BITS)]\n",
    "        w_config_words.reverse()\n",
    "        w_config_words = np.array(w_config_words,dtype=np.int8)\n",
    "        w_config_words = np.repeat(w_config_words[np.newaxis,...],repeats=r.IT,axis=0)\n",
    "\n",
    "        '''Input Config'''\n",
    "        x_config = pack_bits([\n",
    "            (r.KH//2, c.BITS_KH2),\n",
    "            (r.CI-1 , c.BITS_CIN_MAX),\n",
    "            (r.XW-1 , c.BITS_COLS_MAX),\n",
    "            (r.L -1 , c.BITS_BLOCKS_MAX),\n",
    "        ])\n",
    "        assert c.IN_BITS >= c.BITS_KW2 + c.BITS_CIN_MAX + c.BITS_COLS_MAX + c.BITS_BLOCKS_MAX\n",
    "\n",
    "        x_config = format(x_config, f'#0{c.IN_BITS}b')\n",
    "        x_config_words = [int(x_config[i:i+c.X_BITS], 2) for i in range(0, len(x_config), c.X_BITS)]\n",
    "        x_config_words.reverse()\n",
    "\n",
    "        d = {'w_config':w_config, 'w_config_words':w_config_words, 'x_config':x_config, 'x_config_words': x_config_words}\n",
    "        n = namedtuple('Runtime', d)(**d)\n",
    "        r = namedtuple(\"Runtime\", r._fields + n._fields)(*(r + n))\n",
    "        return r\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_sparsity(w, x):\n",
    "        w_sparse = (w==0).sum()/w.size\n",
    "        x_sparse = (x==0).sum()/x.size\n",
    "\n",
    "        p_both_zero = x_sparse * w_sparse\n",
    "        p_only_one_zero = (1-x_sparse) * w_sparse  +  (1-w_sparse) * x_sparse\n",
    "        p_neither_zero = (1-x_sparse) * (1-w_sparse)\n",
    "        zero_result = 1-p_neither_zero\n",
    "\n",
    "        print(f'''\n",
    "        w_sparsity   : {w_sparse*100:.2f}%\n",
    "        x_sparsity   : {x_sparse*100:.2f}%\n",
    "\n",
    "        both_zero    : {p_both_zero*100:.2f}%\n",
    "        only_one_zero: {p_only_one_zero*100:.2f}%\n",
    "        neither_zero : {p_neither_zero*100:.2f}%\n",
    "        zero_result  : {zero_result*100:.2f}%\n",
    "        ''')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reorder_w_q2e_conv(w, c, r):\n",
    "\n",
    "        w = np.pad(w, ((0,0),(0,0),(0,0),(0,r.CO_PAD-r.CO)))        # (KH, KW, CI, CO_PAD)\n",
    "        print(w.shape, (r.KH, r.KW, r.CI, r.IT, r.CO_PRL))\n",
    "        w = w.reshape(r.KH, r.KW, r.CI, r.IT, r.CO_PRL)             # (KH, KW, CI, IT, CO_PRL)\n",
    "        w = np.flip(w, axis=4)\n",
    "        w = w.transpose(0,2,3,4,1)                                  # (KH, CI, IT, CO_PRL, KW)\n",
    "\n",
    "        w = w.reshape  (r.KH, r.CI, r.IT, r.CO_PRL*r.KW)            # (KH, CI, IT, CO_PRL*KW)\n",
    "        w = np.pad(w, ((0,0),(0,0),(0,0),(0,c.COLS-r.CO_PRL*r.KW))) # (KH, CI, IT, c.COLS)\n",
    "        w = w.transpose(2,1,0,3)                                    # (IT, CI, KH, c.COLS)\n",
    "        w = w.reshape (r.IT, r.CI*r.KH, c.COLS)                       # (IT, CI*KH, c.COLS)\n",
    "        \n",
    "        w = np.pad(w, ((0,0),(c.CONFIG_BEATS,0),(0,0)))             # (IT, c.CONFIG_BEATS+CI*KH, c.COLS)\n",
    "        w = w.reshape (r.IT, (r.CI*r.KH+c.CONFIG_BEATS)*c.COLS)     # (IT, (CI*KH+c.CONFIG_BEATS)*c.COLS)\n",
    "\n",
    "        w = np.concatenate([r.w_config_words, w], axis=1)             # (IT, 8 + CI*KH*c.COLS)\n",
    "        assert w.shape == (r.IT, c.IN_BITS/c.K_BITS + (r.CI*r.KH+c.CONFIG_BEATS)*c.COLS)\n",
    "        return w\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reorder_x_q2e_conv(x, c, r):\n",
    "        print('input initial (XN, XH, XW, CI)=', x.shape)\n",
    "\n",
    "        x = np.pad(x, ((0,0),(0,r.XH_PAD-r.XH),(0,0),(0,0)))   # (XN, L*HL , XW, CI)\n",
    "        x = x.reshape  (r.XN, r.L, r.LH, r.XW, r.CI)               # (XN, L, HL, XW, CI)\n",
    "\n",
    "        zeros = np.zeros((r.XN,r.L,c.ROWS+c.X_PAD,r.XW,r.CI),x.dtype)  # (XN,L,c.ROWS+X_PAD,XW,CI)\n",
    "        zeros[:,:,:c.ROWS,:,:] = x\n",
    "\n",
    "        ''' Fill bot rows from next '''\n",
    "        for l in range(r.L):\n",
    "            if l == r.L-1:\n",
    "                zeros[:,l, c.ROWS: ,:,:] = np.zeros((r.XN,c.X_PAD,r.XW,r.CI),x.dtype)\n",
    "            else:\n",
    "                zeros[:,l, c.ROWS: ,:,:] = x[:,l+1,:c.X_PAD,:,:]\n",
    "\n",
    "        x = zeros                  # (XN,L,c.ROWS+X_PAD,XW,CI)\n",
    "        x = x.transpose(0,1,3,4,2) # (XN,L,XW,CI,c.ROWS+X_PAD)\n",
    "\n",
    "        x = x.reshape((r.XN*r.L*r.XW*r.CI*(c.ROWS+c.X_PAD)))\n",
    "        x = np.concatenate([np.array(r.x_config_words, dtype=np.uint8), x.flatten()])\n",
    "        assert x.shape == (c.IN_BITS/c.X_BITS + r.XN*r.L*r.XW*r.CI*(c.ROWS+c.X_PAD),)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reorder_y_q2e_conv(y, c, r):\n",
    "        YH, YW = r.XH_PAD//r.SH_OUT, r.XW//r.SW_OUT\n",
    "\n",
    "        if r.SH_OUT != 1:\n",
    "            print(\"Striding not yet supported\")\n",
    "            return None\n",
    "\n",
    "        y = np.pad(y, ((0,0),(0,r.LH*r.L-r.XH),(0,0),(0,r.CO_PAD-r.CO)))     # (XN, L*HL , XW, CO_PAD)\n",
    "        y = y.reshape((r.XN, r.L, c.ROWS, r.XW, r.CO_PAD))                   # (XN,L,c.ROWS,XW,CO_PAD)\n",
    "        y = y.reshape((r.XN, r.L, c.ROWS, r.XW, r.IT, r.CO_PRL))             # (XN,L,c.ROWS,XW,IT,CO_PRL)\n",
    "        y = y.transpose(4,0,1,3,5,2)                                         # (IT,XN,L,XW,CO_PRL,c.ROWS)\n",
    "\n",
    "        assert y.shape == (r.IT,r.XN,r.L,r.XW,r.CO_PRL,c.ROWS)\n",
    "\n",
    "        y_w_last = y[:,:,:,-(r.KW//2+1):,:,:]\n",
    "        y_w_last = y_w_last.transpose(0,1,2,4,3,5).reshape(r.IT,r.XN,r.L,(r.KW//2+1)*r.CO_PRL,c.ROWS)\n",
    "\n",
    "        y = y.reshape(r.IT,r.XN,r.L,r.XW*r.CO_PRL,c.ROWS)\n",
    "        y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:] = y_w_last\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def reorder_y_e2q_conv(y, c, r):\n",
    "        y = y.reshape(r.IT,r.XN,r.L,r.XW*r.CO_PRL,c.ROWS)\n",
    "\n",
    "        y_w_last = y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:]\n",
    "        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,r.CO_PRL,(r.KW//2+1),c.ROWS)\n",
    "        y_w_last = y_w_last.transpose(0,1,2,4,3,5)   #(r.IT,r.XN,r.L,(r.KW//2+1),r.CO_PRL,c.ROWS)\n",
    "        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,(r.KW//2+1),r.CO_PRL,c.ROWS)\n",
    "        y_w_last = y_w_last.reshape(r.IT,r.XN,r.L,(r.KW//2+1)*r.CO_PRL,c.ROWS)\n",
    "        \n",
    "        y[:,:,:,-(r.KW//2+1)*r.CO_PRL:,:] = y_w_last\n",
    "\n",
    "        y = y.reshape(r.IT,r.XN,r.L,r.XW,r.CO_PRL,c.ROWS)\n",
    "        y = y.transpose(1,2,5,3,0,4)\n",
    "        y = y.reshape((r.XN, r.L*c.ROWS, r.XW, r.CO_PAD))\n",
    "        y = y[:,:r.XH,:,:r.CO]\n",
    "\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def reorder_y_e2e_conv(y, c, r):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def reorder_y_e2e_conv2dense(y, c, r):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 06:35:37.849219: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2023-07-25 06:35:39.378255: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2023-07-25 06:35:40.455834: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2023-07-25 06:35:58.061640: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 8388608 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "with open('bundles.pickle', 'rb') as f:\n",
    "    bundles = pickle.load(f)\n",
    "    \n",
    "for bundle in bundles:\n",
    "    bundle.r = namedtuple('Runtime', bundle.r)(**bundle.r)\n",
    "    bundle.c = namedtuple('Compile', bundle.c)(**bundle.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to site packages: /usr/local/share/pynq-venv/lib64/python3.10/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myOverlay = Overlay('design_1.bit')\n",
    "y_recv = myOverlay.dma_weights_out.recvchannel\n",
    "x_send = myOverlay.dma_pixels.sendchannel\n",
    "w_send = myOverlay.dma_weights_out.sendchannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input initial (XN, XH, XW, CI)= (8, 32, 32, 3)\n",
      "0 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "1 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "2 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "3 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "4 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "5 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "6 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "7 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "8 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "9 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 64)\n",
      "10 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "11 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "12 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "13 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "14 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "15 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "16 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "17 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "18 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "19 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "20 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "21 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "22 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 128)\n",
      "23 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "24 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "25 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "26 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "27 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "28 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "29 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "30 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "31 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "32 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "33 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "34 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "35 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "36 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "37 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "38 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "39 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "40 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "41 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 256)\n",
      "42 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "43 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "44 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "45 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 1024)\n",
      "46 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 2048)\n",
      "47 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "48 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "49 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 2048)\n",
      "50 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "51 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "input initial (XN, XH, XW, CI)= (8, 8, 8, 512)\n",
      "52 ERROR ------------------  0.0 -------------------\n",
      "\n",
      "(8, 32768) (1, 1, 32768, 10)\n",
      "53 True\n"
     ]
    }
   ],
   "source": [
    "xq = bundles[0].x[0]\n",
    "\n",
    "for ib, bundle in enumerate(bundles):\n",
    "    \n",
    "    x_arr = xq if ib==0 else bundle.prev_bundle.chained_output\n",
    "    \n",
    "    if bundle.type == 'conv':\n",
    "        \n",
    "        bundle.xe = bundle.reorder_x_q2e_conv(x_arr, bundle.c, bundle.r)\n",
    "        \n",
    "        '''\n",
    "        RUN ENGINE\n",
    "        '''\n",
    "        w_buf = allocate(shape=(bundle.we[0].shape), dtype=np.int8)\n",
    "        x_buf = allocate(shape=(bundle.xe.shape), dtype=np.int8)\n",
    "        y_buf = allocate(shape=bundle.ye_exp_shape[1:], dtype=np.int32)\n",
    "\n",
    "        for t in range(bundle.num_t):\n",
    "            w_buf[:] = bundle.we[t][:]\n",
    "            x_buf[:] = bundle.xe[:]\n",
    "            y_buf[:] = 0\n",
    "            w_buf.flush()\n",
    "            x_buf.flush()\n",
    "            y_buf.flush()\n",
    "\n",
    "            y_recv.transfer(y_buf)\n",
    "            w_send.transfer(w_buf)\n",
    "            w_send.wait()\n",
    "            x_send.transfer(x_buf)\n",
    "            x_send.wait()\n",
    "            y_buf.invalidate()\n",
    "\n",
    "            bundle.ye_hw[t] = y_buf\n",
    "\n",
    "        bundle.ye_hwr = bundle.reorder_y_e2q_conv(bundle.ye_hw, bundle.c, bundle.r)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        '''\n",
    "        DENSE on CPU\n",
    "        '''\n",
    "        print(x_arr.shape, bundle.w[0].shape)\n",
    "        bundle.ye_hwr = x_arr @ bundle.w[0].reshape(bundle.w[0].shape[2],bundle.w[0].shape[3])\n",
    "    \n",
    "    out = bundle.out = bundle.chained_output = bundle.post_process(bundle.ye_hwr)\n",
    "    \n",
    "    if bundle.softmax:\n",
    "        out = out.reshape(out.shape[0], out.shape[-1])\n",
    "        print(ib, np.all(np.argmax(bundle.o_arr, axis=-1) == np.argmax(out, axis=-1)))\n",
    "    else:\n",
    "        out_f = out/2**bundle.o_frac\n",
    "        print(ib, 'ERROR ------------------ ', np.sum(np.abs(bundle.o_arr - out_f)), '-------------------\\n')       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
