{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_TEST = False\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if QUICK_TEST:\n",
    "  # quick test doesnt work. have to check\n",
    "  KH = KW = 3\n",
    "  CI = 3\n",
    "  CO = 8\n",
    "  N  = 1\n",
    "  XH = 8\n",
    "  XW = 8\n",
    "  X_BITS = 8\n",
    "  K_BITS = 8\n",
    "\n",
    "  import torch\n",
    "  x = torch.from_numpy(np.random.randint(0 , 2**X_BITS-1 ,size=(N,CI,XH,XW)).astype(np.float32))\n",
    "  w = torch.from_numpy(np.random.randint(-2**(K_BITS-1), 2**(K_BITS-1)-1 ,size=(CO,CI,KH,KW)).astype(np.float32))\n",
    "  y = torch.nn.functional.conv2d(x, w, bias=None, stride=1, padding=1, dilation=1, groups=1)\n",
    "\n",
    "  LAYER = {'w':w.numpy().transpose(2,3,1,0), 'x':x.numpy().transpose(0,2,3,1), 'y':y.numpy().transpose(0,2,3,1)}\n",
    "  i_layers = 0\n",
    "  MODEL_NAME = 'test'\n",
    "\n",
    "else:\n",
    "\n",
    "  i_layers = 5\n",
    "  MODEL_NAME = 'vgg16_quant'\n",
    "  with open(f'np/np_dict_{MODEL_NAME}.pickle', 'rb') as f:\n",
    "    LAYERS = pickle.load(f)\n",
    "    LAYER = LAYERS[f'conv_{i_layers}']\n",
    "\n",
    "  X_BITS = 8\n",
    "  K_BITS = 8\n",
    "  IN_BITS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = 'vectors'\n",
    "i_it = 0\n",
    "i_n  = 0\n",
    "\n",
    "ROWS = 8\n",
    "COLS = 24\n",
    "W_CONFIG = 8\n",
    "\n",
    "\n",
    "KW_MAX  = 3\n",
    "KH_MAX  = 3\n",
    "SH_MAX  = 2\n",
    "SW_MAX  = 2\n",
    "CI_MAX  = 1024\n",
    "XW_MAX  = 384\n",
    "XH_MAX  = 256\n",
    "BRAM_WEIGHTS_DEPTH = 1024\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KH=3, KW=3, CI=128, CO=256, CO_PRL=8, EG=8, IT=32, 256\n",
      "weights initial (KH, KW, CI, CO) = (3, 3, 128, 256)\n",
      "initial (XN, XH, XW, CI)= (128, 8, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "w = LAYER['w']\n",
    "x = LAYER['x']\n",
    "\n",
    "SW = 1\n",
    "SH = 1\n",
    "\n",
    "KH, KW, CI, CO = w.shape\n",
    "CO_PRL         = COLS * SW // KW                        # SW cols are processed in parallel\n",
    "EG             = int(np.floor( COLS / (KW + SW - 1)))   # elastic groups\n",
    "IT             = int(np.ceil( CO / (SW*EG)))            # iterations needed\n",
    "CO_PAD         = IT * CO_PRL                            # output cols padded\n",
    "\n",
    "print(f'{KH=}, {KW=}, {CI=}, {CO=}, {CO_PRL=}, {EG=}, {IT=}, {CO_PAD}')\n",
    "print('weights initial (KH, KW, CI, CO) =', w.shape)\n",
    "\n",
    "XN, XH, XW, CI = LAYER['x'].shape\n",
    "print('initial (XN, XH, XW, CI)=', x.shape)\n",
    "\n",
    "LH    = ROWS*SH   # Block height\n",
    "L     = XH//LH    # Blocks\n",
    "L_MAX = XH_MAX//ROWS\n",
    "\n",
    "'''LRELU BEATS'''\n",
    "if KH == 3:\n",
    "  LRELU_BEATS = 9\n",
    "elif KH == 5:\n",
    "  LRELU_BEATS = 9\n",
    "else:\n",
    "  raise \"Error, unsupported KH for lrelu beats\"\n",
    "\n",
    "def clog2(x):\n",
    "  return int(np.ceil(np.log2(x)))\n",
    "\n",
    "BITS_KW2        = clog2((KW_MAX+1)/2)\n",
    "BITS_KH2        = clog2((KH_MAX+1)/2)\n",
    "BITS_SW         = clog2(SW_MAX)\n",
    "BITS_SH         = clog2(SH_MAX)\n",
    "BITS_CIN_MAX    = clog2(CI_MAX)\n",
    "BITS_COLS_MAX   = clog2(XW_MAX)\n",
    "BITS_BLOCKS_MAX = clog2( L_MAX)\n",
    "BITS_BRAM_WEIGHTS_ADDR = clog2(BRAM_WEIGHTS_DEPTH)\n",
    "BRAM_WEIGHTS_ADDR_MAX  = LRELU_BEATS + SW*KH*CI-1\n",
    "\n",
    "'''Shift'''\n",
    "SHIFT = int(np.ceil(KH/SH)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  w_sparsity   : 80.00%\n",
      "  x_sparsity   : 43.20%\n",
      "\n",
      "  both_zero    : 34.56%\n",
      "  only_one_zero: 54.08%\n",
      "  neither_zero : 11.36%\n",
      "  zero_result  : 88.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_sparse = (w==0).sum()/w.size\n",
    "x_sparse = (x==0).sum()/x.size\n",
    "\n",
    "p_both_zero = x_sparse * w_sparse\n",
    "p_only_one_zero = (1-x_sparse) * w_sparse  +  (1-w_sparse) * x_sparse\n",
    "p_neither_zero = (1-x_sparse) * (1-w_sparse)\n",
    "zero_result = 1-p_neither_zero\n",
    "\n",
    "print(f'''\n",
    "  w_sparsity   : {w_sparse*100:.2f}%\n",
    "  x_sparsity   : {x_sparse*100:.2f}%\n",
    "\n",
    "  both_zero    : {p_both_zero*100:.2f}%\n",
    "  only_one_zero: {p_only_one_zero*100:.2f}%\n",
    "  neither_zero : {p_neither_zero*100:.2f}%\n",
    "  zero_result  : {zero_result*100:.2f}%\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights initial (KH, KW, CI, CO) = (3, 3, 128, 256)\n",
      "weights final (IT, 8 + (CI*KH+LRELU_BEATS)*COLS) = (32, 9440) \n",
      "Saved as vectors/vgg16_quant_conv_5_w.txt\n"
     ]
    }
   ],
   "source": [
    "w = LAYER['w']\n",
    "print('weights initial (KH, KW, CI, CO) =', w.shape)\n",
    "\n",
    "w = np.pad(w, ((0,0),(0,0),(0,0),(0,CO_PAD-CO)))   # (KH, KW, CI, CO_PAD)\n",
    "w = w.reshape(KH, KW, CI, IT, CO_PRL)              # (KH, KW, CI, IT, CO_PRL)\n",
    "\n",
    "'''To fix DW bank issue'''\n",
    "RATIO = KW_MAX//KW\n",
    "w = w.reshape  (KH, KW, CI, IT, RATIO, CO_PRL//RATIO)\n",
    "w = w.transpose(0,1,2,3,5,4)                       # (KH, KW, CI, IT, CO_PRL//RATIO, RATIO)\n",
    "w = w.reshape  (KH, KW, CI, IT, CO_PRL)            # (KH, KW, CI, IT, CO_PRL)\n",
    "w = w.transpose(0,2,3,4,1)                         # (KH, CI, IT, CO_PRL, KW)\n",
    "\n",
    "'''Assume SW=1'''\n",
    "CO_PRL    = COLS // KW\n",
    "w = w.reshape  (KH, CI, IT, CO_PRL*KW)                # (KH, CI, IT, CO_PRL*KW)\n",
    "w = np.pad(w, ((0,0),(0,0),(0,0),(0,COLS-CO_PRL*KW))) # (KH, CI, IT, COLS)\n",
    "w = w.transpose(2,1,0,3)                              # (IT, CI, KH, COLS)\n",
    "w = w.reshape (IT, CI*KH, COLS)                       # (IT, CI*KH, COLS)\n",
    "w = np.pad(w, ((0,0),(LRELU_BEATS,0),(0,0)))          # (IT, LRELU_BEATS+CI*KH, COLS)\n",
    "w = w.reshape (IT, (CI*KH+LRELU_BEATS)*COLS)          # (IT, (CI*KH+LRELU_BEATS)*COLS)\n",
    "\n",
    "'''Weights config'''\n",
    "\n",
    "weights_config = 0\n",
    "weights_config |= (KW//2)\n",
    "weights_config |= (KH//2)               << (BITS_KW2)\n",
    "weights_config |= SW-1                  << (BITS_KW2 + BITS_KH2)\n",
    "weights_config |= (CI-1)                << (BITS_KW2 + BITS_KH2 + BITS_SW)\n",
    "weights_config |= (XW-1)                << (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX)\n",
    "weights_config |= ( L-1)                << (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX + BITS_COLS_MAX)\n",
    "weights_config |= BRAM_WEIGHTS_ADDR_MAX << (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX + BITS_COLS_MAX + BITS_BLOCKS_MAX)\n",
    "\n",
    "weights_config = format(weights_config, f'#0{IN_BITS}b')\n",
    "config_words = [int(weights_config[i:i+K_BITS], 2) for i in range(0, len(weights_config), K_BITS)]\n",
    "config_words.reverse()\n",
    "config_words = np.array(config_words,dtype=np.int8)\n",
    "config_words = np.repeat(config_words[np.newaxis,...],repeats=IT,axis=0)\n",
    "'''Final'''\n",
    "w = np.concatenate([config_words, w], axis=1) # (IT, 8 + CI*KH*COLS)\n",
    "assert w.shape == (IT, IN_BITS/K_BITS + (CI*KH+LRELU_BEATS)*COLS)\n",
    "\n",
    "path = f\"{DATA_DIR}/{MODEL_NAME}_conv_{i_layers}_w.txt\"\n",
    "np.savetxt(path, w[i_it].flatten(), fmt='%d')\n",
    "print(f'weights final (IT, 8 + (CI*KH+LRELU_BEATS)*COLS) = {w.shape} \\nSaved as {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input initial (XN, XH, XW, CI)= (128, 8, 8, 128)\n",
      "input final (IN_BITS/X_BITS + L*XW*CI*SH*(ROWS+SHIFT))=(10248,) \n",
      "Saved as \"vectors/vgg16_quant_conv_5_x.txt\"\n"
     ]
    }
   ],
   "source": [
    "x = LAYER['x']\n",
    "print('input initial (XN, XH, XW, CI)=', x.shape)\n",
    "\n",
    "x = np.pad(x, ((0,0),(0,LH*L-XH),(0,0),(0,0)))   # (XN, L*HL , XW, CI)\n",
    "x = x.reshape  (XN, L, LH, XW, CI)               # (XN, L, HL, XW, CI)\n",
    "# x = x.transpose(0,1,3,4,2)                     # (XN, L, HW, CI, HL)\n",
    "\n",
    "zeros = np.zeros((XN,L,SH*(ROWS+SHIFT),XW,CI),x.dtype)  # (XN,L,SH*(ROWS+SHIFT),XW,CI)\n",
    "top_edges = KH//2\n",
    "bot_edges = SH*(ROWS+SHIFT) - top_edges - LH\n",
    "\n",
    "zeros[:,:,top_edges:SH*(ROWS+SHIFT)-bot_edges,:,:] = x\n",
    "\n",
    "for l in range(L):\n",
    "    ''' Fill top rows from prev '''\n",
    "    if l == 0:\n",
    "        zeros[:,l,:top_edges,:,:] = np.zeros((XN,top_edges,XW,CI),x.dtype)\n",
    "    else:\n",
    "        zeros[:,l,:top_edges,:,:] = x[:,l-1,LH-top_edges:,:,:]\n",
    "\n",
    "    ''' Fill bot rows from next '''\n",
    "    if l == L-1:\n",
    "        zeros[:,l,SH*(ROWS+SHIFT)-bot_edges:,:,:] = np.zeros((XN,bot_edges,XW,CI),x.dtype)\n",
    "    else:\n",
    "        zeros[:,l,SH*(ROWS+SHIFT)-bot_edges:,:,:] = x[:,l+1,:bot_edges,:,:]\n",
    "\n",
    "\n",
    "x = zeros                  # (XN,L,SH*(ROWS+SHIFT),XW,CI)\n",
    "x = x.transpose(0,1,3,4,2) # (XN,L,XW,CI,SH*(ROWS+SHIFT))\n",
    "\n",
    "x = x[i_n]\n",
    "x = x.reshape((L*XW*CI*SH*(ROWS+SHIFT)))  #! XN should be moved in\n",
    "\n",
    "'''\n",
    "Config\n",
    "'''\n",
    "is_max     = 0\n",
    "is_not_max = 1\n",
    "is_relu    = 0\n",
    "\n",
    "config = 0\n",
    "config |= is_not_max\n",
    "config |= is_max  << 1\n",
    "config |= is_relu << 2\n",
    "config |= (KH//2) << 3\n",
    "config |= (KW//2) << 3 + BITS_KH2\n",
    "config |= (SH-1 ) << 3 + BITS_KH2 + BITS_KW2\n",
    "config |= (ROWS+SHIFT) << 3 + BITS_KH2 + BITS_KW2 + BITS_SH\n",
    "\n",
    "config = format(config, f'#0{IN_BITS}b')\n",
    "config_words = [int(config[i:i+X_BITS], 2) for i in range(0, len(config), X_BITS)]\n",
    "config_words.reverse()\n",
    "x = np.concatenate([np.array(config_words, dtype=np.uint8), x.flatten()])\n",
    "assert x.shape == (IN_BITS/X_BITS + L*XW*CI*SH*(ROWS+SHIFT),)\n",
    "\n",
    "path = f\"{DATA_DIR}/{MODEL_NAME}_conv_{i_layers}_x.txt\"\n",
    "np.savetxt(path, x.flatten(), fmt='%d')\n",
    "print(f'input final (IN_BITS/X_BITS + L*XW*CI*SH*(ROWS+SHIFT))={x.shape} \\nSaved as \"{path}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Outputs (Conv Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output initial (XN, XH , XW, CO) = (128, 8, 8, 256)\n",
      "output final (IT,XN,L,XW,CO_PRL,ROWS)=(1, 8, 8, 8) \n",
      "Saved as \"vectors/vgg16_quant_conv_5_y_exp.txt\"\n"
     ]
    }
   ],
   "source": [
    "y = LAYER['y']      # (XN, XH , XW, CO)\n",
    "print('output initial (XN, XH , XW, CO) =', y.shape)\n",
    "\n",
    "# '''Flip last XW to imitate conv engine'''\n",
    "# if KW != 1:\n",
    "#   y = np.concatenate([y[:,:,:-(KW-1),:], np.flip(y[:,:,-(KW-1):,:],axis=2)],axis=2)\n",
    "\n",
    "y = np.pad(y, ((0,0),(0,LH*L-XH),(0,0),(0,CO_PAD-CO)))   # (XN, L*HL , XW, CO_PAD)\n",
    "y = y.reshape((XN, L, ROWS, XW, CO_PAD))                 # (XN,L,ROWS,XW,CO_PAD)\n",
    "y = y.reshape((XN, L, ROWS, XW, IT, CO_PRL))             # (XN,L,ROWS,XW,IT,CO_PRL)\n",
    "y = y.transpose(4,0,1,3,5,2)                             # (IT,XN,L,XW,CO_PRL,ROWS)\n",
    "\n",
    "\n",
    "assert y.shape == (IT,XN,L,XW,CO_PRL,ROWS)\n",
    "\n",
    "y = y[0,0]\n",
    "path = f\"{DATA_DIR}/{MODEL_NAME}_conv_{i_layers}_y_exp.txt\"\n",
    "np.savetxt(path, y.flatten(), fmt='%d')\n",
    "print(f'output final (IT,XN,L,XW,CO_PRL,ROWS)={y.shape} \\nSaved as \"{path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sim = np.loadtxt(f\"{DATA_DIR}/{MODEL_NAME}_conv_{i_layers}_y_sim.txt\",np.int32)\n",
    "np.sum(np.abs(y_sim.reshape(y.shape) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
